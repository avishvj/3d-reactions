{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import sklearn.metrics as metrics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem as utils\n",
    "\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "from numpy.random import normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = r'spare_data/'\n",
    "\n",
    "train_r_file = base_folder + 'train_reactants.sdf'\n",
    "train_ts_file = base_folder + 'train_ts.sdf'\n",
    "train_p_file = base_folder + 'train_products.sdf'\n",
    "\n",
    "test_r_file = base_folder + 'test_reactants.sdf'\n",
    "test_ts_file = base_folder + 'test_ts.sdf'\n",
    "test_p_file = base_folder + 'test_products.sdf'\n",
    "\n",
    "train_r = Chem.ForwardSDMolSupplier(train_r_file, removeHs=False, sanitize=False)\n",
    "train_r = [x for x in train_r]\n",
    "train_ts = Chem.ForwardSDMolSupplier(train_ts_file, removeHs=False, sanitize=False)\n",
    "train_ts = [x for x in train_ts]\n",
    "train_p = Chem.ForwardSDMolSupplier(train_p_file, removeHs=False, sanitize=False)\n",
    "train_p = [x for x in train_p]\n",
    "\n",
    "test_r = Chem.ForwardSDMolSupplier(test_r_file, removeHs=False, sanitize=False)\n",
    "test_r = [x for x in test_r]\n",
    "test_ts = Chem.ForwardSDMolSupplier(test_ts_file, removeHs=False, sanitize=False)\n",
    "test_ts = [x for x in test_ts]\n",
    "test_p = Chem.ForwardSDMolSupplier(test_p_file, removeHs=False, sanitize=False)\n",
    "test_p = [x for x in test_p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# restrict number of products created to 30 for testing\n",
    "\n",
    "len(train_r[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# figuring out padding\n",
    "max(mol.GetNumAtoms() for mol in train_r) # = 21, same for ts, p\n",
    "# min(mol.GetNumAtoms() for mol in train_r) # = 4\n",
    "# need to get more\n",
    "\n",
    "# train_r_small = train_r[0:100]\n",
    "# train_ts_small = train_ts[0:100]\n",
    "# train_p_small = train_p[0:100]\n",
    "\n",
    "print(torch.__version__)\n",
    "# do AE\n",
    "# then get to grips with PTG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6, 7, 6, 7, 7, 6, 6, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "train_r_data[9].z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_vae.data_processors.grambow_processor import ReactionDataset\n",
    "\n",
    "# x = atom features; edge_attr = bond types (4 types, one-hot); pos = (x,y,z) coords;  edge_index = graph connectivity; y = list of atomic numbers in mol\n",
    "\n",
    "base_path = r'data/'\n",
    "train_r_data = ReactionDataset(base_path, geo_file = 'train_r') \n",
    "\n",
    "# want Embedding(GNN(MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ReactionDataset(6739)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "train_r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ReLU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Standard MLP. \n",
    "        Reminders because I have no memory: \n",
    "        - MLP with 1 input layer, 1 output layer, and no activation is linear layer.\n",
    "        - DNN = NN with >1 hidden layer. MLP with >1 layer is DNN. \n",
    "        - MLPs are subset of DNN. DNNs can have loops, MLPs are always feed-forward.\n",
    "        - So generally, MLP = FFNN with FC layers and non-linear activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation=ReLU, num_hidden=2):\n",
    "        # may need to add batchnorm and dropout here later\n",
    "        super().__init__()\n",
    "        # fc_layers = fully connected layers\n",
    "        fc_layers = [nn.Linear(input_dim, input_dim) for hidden_idx in range(num_hidden)]\n",
    "        # why this line if already do list above?\n",
    "        fc_layers.append(nn.Linear(input_dim, output_dim))\n",
    "        # sequential here or modulelist? sequential gives ordering, modulelist allows any execution order\n",
    "        self.fc_layers = nn.ModuleList(fc_layers)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for hidden_idx in range(self.num_hidden):\n",
    "            y = self.fc_layers[hidden_idx](y)\n",
    "            y = self.activation(y)\n",
    "        y = self.fc_layers[self.num_hidden](y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "# standard GCN, generalise to taken any convolution\n",
    "class MoleculeGCN(nn.Module):\n",
    "    # could also define task here if wanted to specify embedding vs say node prediction or graph\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim, hidden_layers=2, dropout=0.25, task='node'):\n",
    "        super(MoleculeGCN, self).__init()\n",
    "\n",
    "        # make sure we are performing a possible task\n",
    "        if not (task == 'node' or task == 'graph_classification'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "        self.task = task\n",
    "\n",
    "        # atm, doing convs, could generalise to any layer combination after\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        for layer_idx in range(hidden_layers):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # linear layers post message passing\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 1 + hidden_layers\n",
    "        self.post_mp = nn.Sequential( # should generalise this for number of hidden_layers\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "        # needed?\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def build_model(self, input_dim, hidden_dim):\n",
    "        # currently doing simple node prediction\n",
    "        # can have graph classification later if needed\n",
    "        return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data = self.dataset.data [and then remove from func def]\n",
    "        # x = node feature matrix [num_nodes, num_node_features]\n",
    "        # edge_index = adjacency matrix (sparse adjacency list). what are edges in your graph.\n",
    "        # batch = batching more complicated here since graphs have diff number of nodes (unlike, say, images) so this param records which elem index each node attribute belongs to\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # check: if no features, use constant feature\n",
    "        if data.num_node_features == 0: \n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "        \n",
    "        # execute convolutions over each layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            embedding = x\n",
    "            x = F.relu(x)\n",
    "            # do this because dropout different at train time vs test time\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if not i == (self.num_layers - 1):\n",
    "                x = self.layer_norms[i](x)\n",
    "\n",
    "            # pool nodes then apply message passing layers\n",
    "            # x = pyg_nn.global_mean_pool(x, batch) --> only if graph classification\n",
    "            x = self.post_mp(x)\n",
    "\n",
    "            return embedding, F.log_softmax(x, dim=1)\n",
    "        \n",
    "        def loss(self, pred, label):\n",
    "            return F.nll_loss(pred, label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "train_r_data.num_classes\n",
    "\n",
    "# loose note: could have classes for each type of reaction, uni vs bimolecular, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def train(geom_dataset, task, writer):\n",
    "    num_geometries = len(geom_dataset)\n",
    "    train_loader = DataLoader(geom_dataset[ :int(num_geometries * 0.8)], batch_size=20)\n",
    "    test_loader = DataLoader(geom_dataset[int(num_geometries * 0.8): ], batch_size=20)\n",
    "\n",
    "    # build model and optimiser\n",
    "    model = MoleculeGCN(dataset=geom_dataset, input_dim=max(dataset.num_node_features, 1), hidden_dim=5, output_dim=2, hidden_layers=1)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            embedding, pred = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node':\n",
    "                pred = pred[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()* batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "        \n",
    "        # currently printing for all epochs\n",
    "        test_acc = test(test_loader, model)\n",
    "        print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:4f.}\".format(epoch, total_loss, test_acc))\n",
    "        writer.add_scalar(\"Test accuracy\", test_acc, epoch)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            embedding, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "        if model.task == 'node':\n",
    "            mask = data.val_mask if is_validation else data.test_mask\n",
    "            # node classification: only evaluate nodes in test set\n",
    "            pred = pred[mask]\n",
    "            label = data.y[mask]\n",
    "    \n",
    "    if model.task == 'graph':\n",
    "        total = len(loader.dataset)\n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(\"./log\")\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "from ts_vae.data_processors.grambow_processor import ReactionDataset\n",
    "\n",
    "base_path = r'data/'\n",
    "train_r_dataset = ReactionDataset(base_path, geo_file = 'train_r') \n",
    "task = 'node'\n",
    "model = train(train_r_dataset, task, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear, ReLU\n",
    "# from torch_geometric.nn import Sequential\n",
    "\n",
    "# need to put MLP into this\n",
    "class StandardGNN(nn.Module):\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim):\n",
    "        super(StandardGNN, self).__init__()\n",
    "        # num_features = node features\n",
    "        self.l1 = Linear(dataset.num_features, 6)\n",
    "        self.l2 = Linear(6, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = ReLU(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "class MoleculeEmbedder(nn.Module):\n",
    "    # could also define task here if wanted to specify embedding vs say node prediction or graph\n",
    "    def __init__(self, dataset, hidden_dim, num_hidden=2, activation=ReLU):\n",
    "        super().__init__()\n",
    "        self.gnn = StandardGNN(...) # maybe with MLP here\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim, activation=ReLU, num_hidden=2, dropout=0.25):\n",
    "        super(MoleculeGCNEmbedder, self).__init()\n",
    "\n",
    "        # pass in input_dim, output_dim, activation, num_hidden\n",
    "        self.mlp1 = MLP(input_dim, output_dim, activation, num_hidden)\n",
    "\n",
    "        \n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.lns = nn.ModuleList()\n",
    "        for layer_idx in range(hidden_layers):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # linear layers post message passing\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 1 + hidden_layers\n",
    "        self.post_mp = nn.Sequential( # should generalise this for number of hidden_layers\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def build_model(self, input_dim, hidden_dim):\n",
    "        # currently doing simple node prediction\n",
    "        return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: each graph has atom features, edge features (actually just bond type), connectivity, and coordinates. currently, the y=list of atomic numbers present\n",
    "# may need to define easier dataset for TS prediction with R-P together with y=TS coords\n",
    "# like Gregor RL paper, place atoms \n",
    "\n",
    "# note: can define data class for params e.g. \n",
    "# @dataclass\n",
    "# class GNNParams:\n",
    "#   input_dim: int\n",
    "#   output_dim: int \n",
    "#   ... (hidden_sizes, dropout, batchnorm, activation) \n",
    "\n",
    "# could also have enum for different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['x', 'edge_index', 'edge_attr', 'pos', 'z', 'idx']"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# each graph: num_nodes; num_features/num_node_features; num_edges; num_edge_features; \n",
    "\n",
    "train_r_data[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "num_latent_params = 2 * latent_space_dim\n",
    "network = nn.Sequential(nn.Linear(data_dim, 300), ..., nn.Linear(400, num_latent_params))\n",
    "encoder(network)"
   ]
  }
 ]
}