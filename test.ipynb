{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import sklearn.metrics as metrics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem as utils\n",
    "\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "from numpy.random import normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = r'spare_data/'\n",
    "\n",
    "train_r_file = base_folder + 'train_reactants.sdf'\n",
    "train_ts_file = base_folder + 'train_ts.sdf'\n",
    "train_p_file = base_folder + 'train_products.sdf'\n",
    "\n",
    "test_r_file = base_folder + 'test_reactants.sdf'\n",
    "test_ts_file = base_folder + 'test_ts.sdf'\n",
    "test_p_file = base_folder + 'test_products.sdf'\n",
    "\n",
    "train_r = Chem.ForwardSDMolSupplier(train_r_file, removeHs=False, sanitize=False)\n",
    "train_r = [x for x in train_r]\n",
    "train_ts = Chem.ForwardSDMolSupplier(train_ts_file, removeHs=False, sanitize=False)\n",
    "train_ts = [x for x in train_ts]\n",
    "train_p = Chem.ForwardSDMolSupplier(train_p_file, removeHs=False, sanitize=False)\n",
    "train_p = [x for x in train_p]\n",
    "\n",
    "test_r = Chem.ForwardSDMolSupplier(test_r_file, removeHs=False, sanitize=False)\n",
    "test_r = [x for x in test_r]\n",
    "test_ts = Chem.ForwardSDMolSupplier(test_ts_file, removeHs=False, sanitize=False)\n",
    "test_ts = [x for x in test_ts]\n",
    "test_p = Chem.ForwardSDMolSupplier(test_p_file, removeHs=False, sanitize=False)\n",
    "test_p = [x for x in test_p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# restrict number of products created to 30 for testing\n",
    "\n",
    "len(train_r[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# figuring out padding\n",
    "max(mol.GetNumAtoms() for mol in train_r) # = 21, same for ts, p\n",
    "# min(mol.GetNumAtoms() for mol in train_r) # = 4\n",
    "# need to get more\n",
    "\n",
    "# train_r_small = train_r[0:100]\n",
    "# train_ts_small = train_ts[0:100]\n",
    "# train_p_small = train_p[0:100]\n",
    "\n",
    "print(torch.__version__)\n",
    "# do AE\n",
    "# then get to grips with PTG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([6, 7, 6, 7, 7, 6, 6, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "train_r_data[9].z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_vae.data_processors.grambow_processor import ReactionDataset\n",
    "\n",
    "# x = atom features; edge_attr = bond types (4 types, one-hot); pos = (x,y,z) coords;  edge_index = graph connectivity; y = list of atomic numbers in mol\n",
    "\n",
    "base_path = r'data/'\n",
    "train_r_data = ReactionDataset(base_path, geo_file = 'train_r') \n",
    "\n",
    "# want Embedding(GNN(MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ReLU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Standard MLP. \n",
    "        Reminders because I have no memory: \n",
    "        - MLP with 1 input layer, 1 output layer, and no activation is linear layer.\n",
    "        - DNN = NN with >1 hidden layer. MLP with >1 layer is DNN. \n",
    "        - MLPs are subset of DNN. DNNs can have loops, MLPs are always feed-forward.\n",
    "        - So generally, MLP = FFNN with FC layers and non-linear activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation=ReLU, num_hidden=2):\n",
    "        super().__init__()\n",
    "        # fc_layers = fully connected layers\n",
    "        fc_layers = [nn.Linear(input_dim, input_dim) for hidden_idx in range(num_hidden)]\n",
    "        fc_layers.append(nn.Linear(input_dim, output_dim))\n",
    "        self.fc_layers = nn.ModuleList(fc_layers)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for hidden_idx in range(self.num_hidden):\n",
    "            y = self.fc_layers[hidden_idx](y)\n",
    "            y = self.activation(y)\n",
    "        y = self.fc_layers[self.num_hidden](y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "# standard GCN, generalise to taken any convolution\n",
    "class MoleculeGCN(nn.Module):\n",
    "    # could also define task here if wanted to specify embedding vs say node prediction or graph\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim, hidden_layers=2, dropout=0.25):\n",
    "        super(MoleculeGCN, self).__init()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.lns = nn.ModuleList()\n",
    "        for layer_idx in range(hidden_layers):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # linear layers post message passing\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 1 + hidden_layers\n",
    "        self.post_mp = nn.Sequential( # should generalise this for number of hidden_layers\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def build_model(self, input_dim, hidden_dim):\n",
    "        # currently doing simple node prediction\n",
    "        return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear, ReLU\n",
    "# from torch_geometric.nn import Sequential\n",
    "\n",
    "# need to put MLP into this\n",
    "class StandardGNN(nn.Module):\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim):\n",
    "        super(StandardGNN, self).__init__()\n",
    "        # num_features = node features\n",
    "        self.l1 = Linear(dataset.num_features, 6)\n",
    "        self.l2 = Linear(6, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = ReLU(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "class MoleculeEmbedder(nn.Module):\n",
    "    # could also define task here if wanted to specify embedding vs say node prediction or graph\n",
    "    def __init__(self, dataset, hidden_dim, num_hidden=2, activation=ReLU):\n",
    "        super().__init__()\n",
    "        self.gnn = StandardGNN(...) # maybe with MLP here\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim, activation=ReLU, num_hidden=2, dropout=0.25):\n",
    "        super(MoleculeGCNEmbedder, self).__init()\n",
    "\n",
    "        # pass in input_dim, output_dim, activation, num_hidden\n",
    "        self.mlp1 = MLP(input_dim, output_dim, activation, num_hidden)\n",
    "\n",
    "        \n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.lns = nn.ModuleList()\n",
    "        for layer_idx in range(hidden_layers):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # linear layers post message passing\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 1 + hidden_layers\n",
    "        self.post_mp = nn.Sequential( # should generalise this for number of hidden_layers\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def build_model(self, input_dim, hidden_dim):\n",
    "        # currently doing simple node prediction\n",
    "        return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: each graph has atom features, edge features (actually just bond type), connectivity, and coordinates. currently, the y=list of atomic numbers present\n",
    "# may need to define easier dataset for TS prediction with R-P together with y=TS coords\n",
    "# like Gregor RL paper, place atoms \n",
    "\n",
    "# note: can define data class for params e.g. \n",
    "# @dataclass\n",
    "# class GNNParams:\n",
    "#   input_dim: int\n",
    "#   output_dim: int \n",
    "#   ... (hidden_sizes, dropout, batchnorm, activation) \n",
    "\n",
    "# could also have enum for different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['x', 'edge_index', 'edge_attr', 'pos', 'z', 'idx']"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# each graph: num_nodes; num_features/num_node_features; num_edges; num_edge_features; \n",
    "\n",
    "train_r_data[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "num_latent_params = 2 * latent_space_dim\n",
    "network = nn.Sequential(nn.Linear(data_dim, 300), ..., nn.Linear(400, num_latent_params))\n",
    "encoder(network)"
   ]
  }
 ]
}