{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "import sklearn.metrics as metrics\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem as utils\n",
    "\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "from numpy.random import normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = r'spare_data/'\n",
    "\n",
    "train_r_file = base_folder + 'train_reactants.sdf'\n",
    "train_ts_file = base_folder + 'train_ts.sdf'\n",
    "train_p_file = base_folder + 'train_products.sdf'\n",
    "\n",
    "test_r_file = base_folder + 'test_reactants.sdf'\n",
    "test_ts_file = base_folder + 'test_ts.sdf'\n",
    "test_p_file = base_folder + 'test_products.sdf'\n",
    "\n",
    "train_r = Chem.ForwardSDMolSupplier(train_r_file, removeHs=False, sanitize=False)\n",
    "train_r = [x for x in train_r]\n",
    "train_ts = Chem.ForwardSDMolSupplier(train_ts_file, removeHs=False, sanitize=False)\n",
    "train_ts = [x for x in train_ts]\n",
    "train_p = Chem.ForwardSDMolSupplier(train_p_file, removeHs=False, sanitize=False)\n",
    "train_p = [x for x in train_p]\n",
    "\n",
    "test_r = Chem.ForwardSDMolSupplier(test_r_file, removeHs=False, sanitize=False)\n",
    "test_r = [x for x in test_r]\n",
    "test_ts = Chem.ForwardSDMolSupplier(test_ts_file, removeHs=False, sanitize=False)\n",
    "test_ts = [x for x in test_ts]\n",
    "test_p = Chem.ForwardSDMolSupplier(test_p_file, removeHs=False, sanitize=False)\n",
    "test_p = [x for x in test_p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "# restrict number of products created to 30 for testing\n",
    "\n",
    "len(train_r[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# figuring out padding\n",
    "max(mol.GetNumAtoms() for mol in train_r) # = 21, same for ts, p\n",
    "# min(mol.GetNumAtoms() for mol in train_r) # = 4\n",
    "# need to get more\n",
    "\n",
    "# train_r_small = train_r[0:100]\n",
    "# train_ts_small = train_ts[0:100]\n",
    "# train_p_small = train_p[0:100]\n",
    "\n",
    "print(torch.__version__)\n",
    "# do AE\n",
    "# then get to grips with PTG\n",
    "\n",
    "train_r_data.num_classes\n",
    "\n",
    "# loose note: could have classes for each type of reaction, uni vs bimolecular, etc.\n",
    "\n",
    "# should I normalise the targets to mean=0, std=1 like in qm9_nn_conv.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_vae.data_processors.grambow_processor import ReactionDataset\n",
    "\n",
    "# x = atom features; edge_attr = bond types (4 types, one-hot); pos = (x,y,z) coords;  edge_index = graph connectivity; y = list of atomic numbers in mol\n",
    "\n",
    "base_path = r'data/'\n",
    "train_r_data = ReactionDataset(base_path, geo_file = 'train_r') \n",
    "\n",
    "# want Embedding(GNN(MLP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ReactionDataset(6739)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "train_r_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import ReLU\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\" Standard MLP. \n",
    "        Reminders because I have no memory: \n",
    "        - MLP with 1 input layer, 1 output layer, and no activation is linear layer.\n",
    "        - DNN = NN with >1 hidden layer. MLP with >1 layer is DNN. \n",
    "        - MLPs are subset of DNN. DNNs can have loops, MLPs are always feed-forward.\n",
    "        - So generally, MLP = FFNN with FC layers and non-linear activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, activation=ReLU, num_hidden=2):\n",
    "        # may need to add batchnorm and dropout here later\n",
    "        super().__init__()\n",
    "        # fc_layers = fully connected layers\n",
    "        fc_layers = [nn.Linear(input_dim, input_dim) for hidden_idx in range(num_hidden)]\n",
    "        # why this line if already do list above?\n",
    "        fc_layers.append(nn.Linear(input_dim, output_dim))\n",
    "        # sequential here or modulelist? sequential gives ordering, modulelist allows any execution order\n",
    "        self.fc_layers = nn.ModuleList(fc_layers)\n",
    "        self.num_hidden = num_hidden\n",
    "        self.activation = activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for hidden_idx in range(self.num_hidden):\n",
    "            y = self.fc_layers[hidden_idx](y)\n",
    "            y = self.activation(y)\n",
    "        y = self.fc_layers[self.num_hidden](y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# standard GCN, generalise to taken any convolution\n",
    "# define task later\n",
    "class MoleculeGCN(nn.Module):\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim, hidden_layers=2, dropout=0.25, task='node'):\n",
    "        super().__init__()\n",
    "\n",
    "        # make sure we are performing a possible task\n",
    "        if not (task == 'node' or task == 'graph_classification'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "        self.task = task\n",
    "\n",
    "        # atm, doing convs, could generalise to any layer combination after\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_model(input_dim, hidden_dim))\n",
    "        self.layer_norms = nn.ModuleList()\n",
    "        for layer_idx in range(hidden_layers):\n",
    "            self.convs.append(self.build_model(hidden_dim, hidden_dim))\n",
    "            self.layer_norms.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # linear layers post message passing\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 1 + hidden_layers\n",
    "        self.post_mp = nn.Sequential( # should generalise this for number of hidden_layers\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "        # needed?\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def build_model(self, input_dim, hidden_dim):\n",
    "        # currently doing simple node prediction\n",
    "        # can have graph classification later if needed\n",
    "        return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data = self.dataset.data [and then remove from func def]\n",
    "        # x = node feature matrix [num_nodes, num_node_features]\n",
    "        # edge_index = adjacency matrix (sparse adjacency list). what are edges in your graph.\n",
    "        # batch = batching more complicated here since graphs have diff number of nodes (unlike, say, images) so this param records which elem index each node attribute belongs to\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # check: if no features, use constant feature\n",
    "        if data.num_node_features == 0: \n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "        \n",
    "        # execute convolutions over each layer\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            embedding = x\n",
    "            x = F.relu(x)\n",
    "            # do this because dropout different at train time vs test time\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if not i == (self.num_layers - 1):\n",
    "                x = self.layer_norms[i](x)\n",
    "\n",
    "            # pool nodes then apply message passing layers\n",
    "            # x = pyg_nn.global_mean_pool(x, batch) --> only if graph classification\n",
    "            x = self.post_mp(x)\n",
    "\n",
    "            # return embedding to visualise later\n",
    "            return embedding, F.log_softmax(x, dim=1)\n",
    "        \n",
    "        def loss(self, pred, label):\n",
    "            # negative log likelihood\n",
    "            return F.nll_loss(pred, label)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'geom_dataset' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-3473b5b3496c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# build loaders before passing in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeom_dataset\u001b[0m\u001b[1;33m[\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_geometries\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeom_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_geometries\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# build model and optimiser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'geom_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# build loaders before passing in\n",
    "train_loader = DataLoader(geom_dataset[ :int(num_geometries * 0.8)], batch_size = train_batch_size)\n",
    "test_loader = DataLoader(geom_dataset[int(num_geometries * 0.8): ], batch_size = test_batch_size)\n",
    "\n",
    "# build model and optimiser\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MoleculeGCN(dataset=geom_dataset, input_dim=max(geom_dataset.num_node_features, 1), hidden_dim=5, output_dim=2, hidden_layers=1).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([6, 6, 6,  ..., 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(r_dataset.data.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Can only calculate the mean of floating types. Got Long instead.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-668444339bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Normalize targets to mean = 0 and std = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mr_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mr_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can only calculate the mean of floating types. Got Long instead."
     ]
    }
   ],
   "source": [
    "# SHOULD CHANGE THIS Z TO Y\n",
    "\n",
    "# Normalize targets to mean = 0 and std = 1.\n",
    "mean = r_dataset.data.z.mean(dim = 0, keepdim = True)\n",
    "std = r_dataset.data.z.std(dim = 0, keepdim = True)\n",
    "r_dataset.data.z = (r_dataset.data.z - mean) / std\n",
    "mean, std = mean[:, target].item(), std[:, target].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    \n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        opt.zero_grad()\n",
    "        # compare model results with expected to get loss\n",
    "        loss = model.loss(model(data), data.y)\n",
    "        loss.backward()\n",
    "        loss_all += loss.item() * data.num_graphs # data.num_graphs = 1 but can change for diff batch size later\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_all / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "\n",
    "def test(loader):\n",
    "    \n",
    "    model.eval()\n",
    "    error = 0\n",
    "\n",
    "    # need to swap for binary CE\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        error += (model(data) * std - data.y * std).abs().sum().item()  # MAE\n",
    "\n",
    "    return error / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_error = None\n",
    "for epoch in range(1, 3):\n",
    "    lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "    loss = train(epoch)\n",
    "    test_error = test(test_loader)\n",
    "\n",
    "    print('Epoch: {:03d}, LR: {:7f}, Loss: {:.7f}, Test MAE: {:.7f}'.format(epoch, lr, loss, test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1111111111"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm\n",
    "num_geometries = len(r_dataset)\n",
    "train_loader = DataLoader(r_dataset[ :int(num_geometries * 0.1)])\n",
    "for i, data in enumerate(train_loader):\n",
    "    print(data.num_graphs, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "def train(geom_dataset, task, writer, train_batch_size, test_batch_size):\n",
    "    num_geometries = len(geom_dataset)\n",
    "    train_loader = DataLoader(geom_dataset[ :int(num_geometries * 0.8)], batch_size=train_batch_size)\n",
    "    test_loader = DataLoader(geom_dataset[int(num_geometries * 0.8): ], batch_size=test_batch_size)\n",
    "\n",
    "    # build model and optimiser\n",
    "    model = MoleculeGCN(dataset=geom_dataset, input_dim=max(geom_dataset.num_node_features, 1), hidden_dim=5, output_dim=2, hidden_layers=1)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    # train\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            opt.zero_grad()\n",
    "            embedding, pred = model(batch)\n",
    "            label = batch.y\n",
    "            if task == 'node':\n",
    "                pred = pred[batch.train_mask]\n",
    "                label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()* batch.num_graphs\n",
    "        total_loss /= len(train_loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "        \n",
    "        # currently printing for all epochs\n",
    "        test_acc = test(test_loader, model)\n",
    "        print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:4f.}\".format(epoch, total_loss, test_acc))\n",
    "        writer.add_scalar(\"Test accuracy\", test_acc, epoch)\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            embedding, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "        if model.task == 'node':\n",
    "            mask = data.test_mask\n",
    "            # node classification: only evaluate nodes in test set\n",
    "            pred = pred[mask]\n",
    "            label = data.y[mask]\n",
    "    \n",
    "    #if model.task == 'graph':\n",
    "    #    total = len(loader.dataset)\n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            total += torch.sum(data.test_mask).item()\n",
    "        return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
    "    .format(\"./log\")\n",
    ")\n",
    "get_ipython().system_raw('./ngrok http 6006 &')\n",
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from ts_vae.data_processors.grambow_processor import ReactionDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "base_path = r'data/'\n",
    "r_dataset = ReactionDataset(base_path, geo_file = 'train_r') \n",
    "task = 'node'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_data.train_mask = data.test_mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'train_mask'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-88a2bd35e503>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-0140698e4f5a>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(geom_dataset, task, writer, train_batch_size, test_batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'node'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Batch' object has no attribute 'train_mask'"
     ]
    }
   ],
   "source": [
    "model = train(r_dataset, task, writer, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear, ReLU\n",
    "# from torch_geometric.nn import Sequential\n",
    "\n",
    "# need to put MLP into this\n",
    "class StandardGNN(nn.Module):\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim):\n",
    "        super(StandardGNN, self).__init__()\n",
    "        # num_features = node features\n",
    "        self.l1 = Linear(dataset.num_features, 6)\n",
    "        self.l2 = Linear(6, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = ReLU(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "class MoleculeEmbedder(nn.Module):\n",
    "    # could also define task here if wanted to specify embedding vs say node prediction or graph\n",
    "    def __init__(self, dataset, hidden_dim, num_hidden=2, activation=ReLU):\n",
    "        super().__init__()\n",
    "        self.gnn = StandardGNN(...) # maybe with MLP here\n",
    "\n",
    "\n",
    "\n",
    "    def __init__(self, dataset, input_dim, hidden_dim, output_dim, activation=ReLU, num_hidden=2, dropout=0.25):\n",
    "        super(MoleculeGCNEmbedder, self).__init()\n",
    "\n",
    "        # pass in input_dim, output_dim, activation, num_hidden\n",
    "        self.mlp1 = MLP(input_dim, output_dim, activation, num_hidden)\n",
    "\n",
    "        \n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.lns = nn.ModuleList()\n",
    "        for layer_idx in range(hidden_layers):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "            self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        \n",
    "        # linear layers post message passing\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = 1 + hidden_layers\n",
    "        self.post_mp = nn.Sequential( # should generalise this for number of hidden_layers\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def build_model(self, input_dim, hidden_dim):\n",
    "        # currently doing simple node prediction\n",
    "        return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: each graph has atom features, edge features (actually just bond type), connectivity, and coordinates. currently, the y=list of atomic numbers present\n",
    "# may need to define easier dataset for TS prediction with R-P together with y=TS coords\n",
    "# like Gregor RL paper, place atoms \n",
    "\n",
    "# note: can define data class for params e.g. \n",
    "# @dataclass\n",
    "# class GNNParams:\n",
    "#   input_dim: int\n",
    "#   output_dim: int \n",
    "#   ... (hidden_sizes, dropout, batchnorm, activation) \n",
    "\n",
    "# could also have enum for different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['x', 'edge_index', 'edge_attr', 'pos', 'z', 'idx']"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "# each graph: num_nodes; num_features/num_node_features; num_edges; num_edge_features; \n",
    "\n",
    "train_r_data[0].keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "num_latent_params = 2 * latent_space_dim\n",
    "network = nn.Sequential(nn.Linear(data_dim, 300), ..., nn.Linear(400, num_latent_params))\n",
    "encoder(network)"
   ]
  }
 ]
}