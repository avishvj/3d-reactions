{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual data processing if issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.core.fromnumeric import product\r\n",
    "from scipy.sparse import data\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch_scatter import scatter\r\n",
    "from torch_geometric.data import InMemoryDataset, DataLoader # , Data\r\n",
    "from torch_geometric.data.data import Data\r\n",
    "from rdkit import Chem\r\n",
    "from rdkit.Chem.rdchem import HybridizationType\r\n",
    "from rdkit.Chem.rdchem import BondType as BT\r\n",
    "from tqdm import tqdm\r\n",
    "\r\n",
    "def process_geometry_file(geometry_file, list = None):\r\n",
    "    \"\"\" Code mostly lifted from QM9 dataset creation https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/qm9.html \r\n",
    "        Transforms molecules to their atom features and adjacency lists.\r\n",
    "    \"\"\"\r\n",
    "    types = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\r\n",
    "    bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\r\n",
    "    limit = 100\r\n",
    "\r\n",
    "    data_list = list if list else []\r\n",
    "    full_path = r'data' + geometry_file\r\n",
    "    geometries = Chem.SDMolSupplier(full_path, removeHs=False, sanitize=False)\r\n",
    "\r\n",
    "    # get atom and edge features for each geometry\r\n",
    "    for i, mol in enumerate(tqdm(geometries)):\r\n",
    "\r\n",
    "        # temp soln cos of split edge memory issues\r\n",
    "        if i == limit:\r\n",
    "            break\r\n",
    "        \r\n",
    "        N = mol.GetNumAtoms()\r\n",
    "        # get atom positions as matrix w shape [num_nodes, num_dimensions] = [num_atoms, 3]\r\n",
    "        atom_data = geometries.GetItemText(i).split('\\n')[4:4 + N] \r\n",
    "        atom_positions = [[float(x) for x in line.split()[:3]] for line in atom_data]\r\n",
    "        atom_positions = torch.tensor(atom_positions, dtype=torch.float)\r\n",
    "        # all the features\r\n",
    "        type_idx = []\r\n",
    "        atomic_number = []\r\n",
    "        aromatic = []\r\n",
    "        sp = []\r\n",
    "        sp2 = []\r\n",
    "        sp3 = []\r\n",
    "        num_hs = []\r\n",
    "\r\n",
    "        # atom/node features\r\n",
    "        for atom in mol.GetAtoms():\r\n",
    "            type_idx.append(types[atom.GetSymbol()])\r\n",
    "            atomic_number.append(atom.GetAtomicNum())\r\n",
    "            aromatic.append(1 if atom.GetIsAromatic() else 0)\r\n",
    "            hybridisation = atom.GetHybridization()\r\n",
    "            sp.append(1 if hybridisation == HybridizationType.SP else 0)\r\n",
    "            sp2.append(1 if hybridisation == HybridizationType.SP2 else 0)\r\n",
    "            sp3.append(1 if hybridisation == HybridizationType.SP3 else 0)\r\n",
    "            # !!! should do the features that lucky does: whether bonded, 3d_rbf\r\n",
    "\r\n",
    "        # bond/edge features\r\n",
    "        row, col, edge_type = [], [], []\r\n",
    "        for bond in mol.GetBonds(): \r\n",
    "            start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\r\n",
    "            row += [start, end]\r\n",
    "            col += [end, start]\r\n",
    "            # edge type for each bond type; *2 because both ways\r\n",
    "            edge_type += 2 * [bonds[bond.GetBondType()]]\r\n",
    "        # edge_index is graph connectivity in COO format with shape [2, num_edges]\r\n",
    "        edge_index = torch.tensor([row, col], dtype=torch.long)\r\n",
    "        edge_type = torch.tensor(edge_type, dtype=torch.long)\r\n",
    "        # edge_attr is edge feature matrix with shape [num_edges, num_edge_features]\r\n",
    "        edge_attr = F.one_hot(edge_type, num_classes=len(bonds)).to(torch.float) \r\n",
    "\r\n",
    "        # order edges based on combined ascending order\r\n",
    "        perm = (edge_index[0] * N + edge_index[1]).argsort() # TODO\r\n",
    "        edge_index = edge_index[:, perm]\r\n",
    "        edge_type = edge_type[perm]\r\n",
    "        edge_attr = edge_attr[perm]\r\n",
    "\r\n",
    "        row, col = edge_index\r\n",
    "        z = torch.tensor(atomic_number, dtype=torch.long)\r\n",
    "        hs = (z == 1).to(torch.float) # hydrogens\r\n",
    "        num_hs = scatter(hs[row], col, dim_size=N).tolist() # scatter helps with one-hot\r\n",
    "        \r\n",
    "        x1 = F.one_hot(torch.tensor(type_idx), num_classes=len(types))\r\n",
    "        x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, num_hs], dtype=torch.float).t().contiguous()\r\n",
    "        x = torch.cat([x1.to(torch.float), x2], dim=-1)\r\n",
    "\r\n",
    "        data = Data(x=x, z=z, pos=atom_positions, edge_index=edge_index, edge_attr=edge_attr, idx=i)\r\n",
    "        \r\n",
    "        data_list.append(data)\r\n",
    "\r\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 100/6739 [00:00<00:23, 286.92it/s]\n",
      " 12%|█▏        | 100/842 [00:00<00:02, 268.14it/s]\n",
      "  1%|▏         | 100/6739 [00:00<00:16, 414.71it/s]\n",
      " 12%|█▏        | 100/842 [00:00<00:01, 404.30it/s]\n",
      "  1%|▏         | 100/6739 [00:00<00:12, 548.55it/s]\n",
      " 12%|█▏        | 100/842 [00:00<00:02, 314.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.data.data.Data'> <class 'torch_geometric.data.data.Data'> <class 'torch_geometric.data.data.Data'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# concat train r and test r\r\n",
    "reactants = []\r\n",
    "reactants = process_geometry_file('/raw/train_reactants.sdf', reactants)\r\n",
    "reactants = process_geometry_file('/raw/test_reactants.sdf', reactants)\r\n",
    "\r\n",
    "# concat train ts and test ts\r\n",
    "ts = []\r\n",
    "ts = process_geometry_file('/raw/train_ts.sdf', ts)\r\n",
    "ts = process_geometry_file('/raw/test_ts.sdf', ts) \r\n",
    "\r\n",
    "# concat train p and test p\r\n",
    "products = []\r\n",
    "products = process_geometry_file('/raw/train_products.sdf', products)\r\n",
    "products = process_geometry_file('/raw/test_products.sdf', products) \r\n",
    "\r\n",
    "assert len(reactants) == len(ts) == len(products)\r\n",
    "\r\n",
    "print(type(reactants[0]), type(ts[0]), type(products[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionTriple(Data):\r\n",
    "    def __init__(self, r = None, ts = None, p = None):\r\n",
    "        super(ReactionTriple, self).__init__()\r\n",
    "        self.r = r\r\n",
    "        self.ts = ts\r\n",
    "        self.p = p\r\n",
    "\r\n",
    "    def __inc__(self, key, value):\r\n",
    "        if key == 'r':\r\n",
    "            return self.r.edge_index.size(0)\r\n",
    "        elif key == 'ts':\r\n",
    "            return self.ts.edge_index.size(0)\r\n",
    "        elif key == 'p':\r\n",
    "            return self.p.edge_index.size(0)\r\n",
    "        else:\r\n",
    "            return super().__inc__(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rxns = []\r\n",
    "for rxn_id in range(len(reactants)):\r\n",
    "    rxn = ReactionTriple(reactants[rxn_id], ts[rxn_id], products[rxn_id])\r\n",
    "    rxns.append(rxn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge2adj(z, edge_index, sigmoid = True):\r\n",
    "    value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim = 1)\r\n",
    "    return torch.sigmoid(value) if sigmoid else value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "26"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# their model \r\n",
    "# so they take their nodes, edges, edge_attr and actual adj\r\n",
    "# adj_pred, z = model(nodes, edges, edge_attr)\r\n",
    "# bce, kl = loss(adj_pred, adj_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj\r\n",
    "\r\n",
    "node_fs = mol_graph.x\r\n",
    "edge_index = mol_graph.edge_index\r\n",
    "edge_attr = mol_graph.edge_attr\r\n",
    "num_nodes = len(mol_graph.z)\r\n",
    "latent_dim = 3\r\n",
    "max_num_nodes = 21\r\n",
    "\r\n",
    "def sparse_to_dense_adj(num_nodes, edge_index):\r\n",
    "    # edge_index is sparse_adj matrix (given in coo format for graph connectivity)\r\n",
    "    sparse_adj = torch.cat([edge_index[0].unsqueeze(0), edge_index[1].unsqueeze(0)])\r\n",
    "    # the values we put in at each tuple; that's why length of sparse_adj\r\n",
    "    ones = torch.ones(sparse_adj.size(1)) \r\n",
    "    # FloatTensor() creates sparse coo tensor in torch format, then to_dense()\r\n",
    "    dense_adj = torch.sparse.FloatTensor(sparse_adj, ones, torch.Size([num_nodes, num_nodes])).to_dense() # to_dense adds the zeroes needed\r\n",
    "    return dense_adj\r\n",
    "\r\n",
    "\r\n",
    "adj_egnn = sparse_to_dense_adj(num_nodes, edge_index)\r\n",
    "# with edge_attr, we get a [1, num_nodes, num_nodes] for each edge_type\r\n",
    "adj_pyg = to_dense_adj(edge_index, edge_attr = edge_attr, max_num_nodes = num_nodes)\r\n",
    "\r\n",
    "# get_dense_graph(): returns self.nodes, self.edges_dense, self.edge_attr_dense, self.adj\r\n",
    "# adj = sparse2dense(n_nodes, self.edges); adjust for loops\r\n",
    "# compare sparse2dense (egnn) vs to_dense_adj (pyg)\r\n",
    "\r\n",
    "# adj_egnn.shape\r\n",
    "# (adj_pyg == adj_egnn).all()\r\n",
    "\r\n",
    "# gcn = GCNConv(num_nodes, latent_dim)\r\n",
    "# z = gcn(node_fs, edge_index)\r\n",
    "\r\n",
    "# adj_pred = adj_pred * (1 - torch.eye(num_nodes).to(self.device)) # removes self_loops\r\n",
    "# * is hadamard product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coords always same, maybe node and edge features too? need to pad adj matrix\r\n",
    "\r\n",
    "# dataset dims\r\n",
    "elements = \"HCNO\"\r\n",
    "num_elements = len(elements)\r\n",
    "max_n_atoms = max([r.GetNumAtoms() for r,ts,p in data])\r\n",
    "num_coords = 3\r\n",
    "num_bond_fs\r\n",
    "\r\n",
    "# want to pad exist features\r\n",
    "\r\n",
    "def prepare_batch(batch_mols):\r\n",
    "\r\n",
    "    # initialise batch\r\n",
    "    batch_size = len(batch_mols)\r\n",
    "    atom_fs = torch.zeros((batch_size, max_n_atoms, num_elements + 1), dtype = torch.float32) # num_atoms, max_num_atoms, \r\n",
    "    bond_fs = torch.zeros((batch_size, max_n_atoms, max_n_atoms, num_bond_fs), dtype = torch.float32)\r\n",
    "    sizes = torch.zeros(batch_size, dtype = torch.float32)\r\n",
    "    coords = torch.zeros((batch_size, max_size, num_coords), dtype = torch.float32)\r\n",
    "    \r\n",
    "    pass\r\n",
    "\r\n",
    "def pad_sequence(sequences: List[torch.Tensor], max_length: int, padding_value=0) -> torch.Tensor:\r\n",
    "    # assuming trailing dimensions and type of all the Tensors\r\n",
    "    # in sequences are same and fetching those from sequences[0]\r\n",
    "    max_size = sequences[0].size()\r\n",
    "    trailing_dims = max_size[1:]\r\n",
    "    out_dims = (len(sequences), max_length) + trailing_dims\r\n",
    "\r\n",
    "    out_tensor = sequences[0].data.new(*out_dims).fill_(padding_value)  # type: ignore\r\n",
    "    for i, tensor in enumerate(sequences):\r\n",
    "        length = tensor.size(0)\r\n",
    "        # use index notation to prevent duplicate references to the tensor\r\n",
    "        out_tensor[i, :length, ...] = tensor\r\n",
    "\r\n",
    "    return out_tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit (conda)",
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}