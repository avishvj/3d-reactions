{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from numpy import exp, sqrt\n",
    "from numpy.random import normal\n",
    "from torch import exp, sqrt, randn_like\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 4      # arbitrary\n",
    "MAX_SIZE = 22*3     # try first with (x,y,z) for each atom\n",
    "\n",
    "class MoleculeVAE(nn.Module):\n",
    "    def __init__(self): # maybe define encoder, decoder, in_channels, out_channels in constructor too\n",
    "        # self.decoder = DefaultDecoder() if decoder is None else decoder\n",
    "        # maybe pass in data as reaction frame here too?\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(MAX_SIZE, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2*LATENT_DIM)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2*LATENT_DIM, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, MAX_SIZE)\n",
    "        )\n",
    "    \n",
    "    def reparameterise(self, mean_z, log_var_z):\n",
    "        if self.training:\n",
    "            eps = randn_like() # is this meant to be randn_like(log_var_z)?\n",
    "            z = mean_z + eps * sqrt(exp(log_var_z))\n",
    "            return z\n",
    "        else:\n",
    "            return mean_z\n",
    "    \n",
    "    def forward(self, x): # note: might be easier to do this the other way explicitly defining mean and var\n",
    "        # reshape input into a vector, then reshape using view(-1, batchsize=2, d)\n",
    "        params_z = self.encoder(x.view(-1, input_size)).view(-1, 2, LATENT_DIM) # this encoder may need to be changed\n",
    "        mean_z = params_z[:, 0, :]\n",
    "        log_var_z = params_z[:, 1, :]\n",
    "        z = self.reparameterise(mean_z, log_var_z)\n",
    "        return self.decoder(z), mean_z, log_var_z \n",
    "\n",
    "    # x_hat = model TS; x = TS\n",
    "    def loss_func(generated_TS, real_TS, mean_z, log_var_z, beta=1):\n",
    "        BCE = nn.functional.binary_cross_entropy(generated_TS, real_TS.view(-1, MAX_SIZE), reduction='sum') \n",
    "        KLD = 0.5 * torch.sum(exp(log_var_z) - log_var_z - 1 + mean_z**2)\n",
    "        return BCE + beta * KLD\n",
    "\n",
    "# define device and model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MoleculeVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in cell below, need to figure out how to load sdf data into loader\n",
    "# look at pytorch data handling https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "\n",
    "# load train and test data and convert to RDKit mols in a list\n",
    "base_folder = 'data/'\n",
    "train_r_file = base_folder + 'train_reactants.sdf'\n",
    "train_ts_file = base_folder + 'train_ts.sdf'\n",
    "train_p_file = base_folder + 'train_products.sdf'\n",
    "test_r_file = base_folder + 'test_reactants.sdf'\n",
    "test_ts_file = base_folder + 'test_ts.sdf'\n",
    "test_p_file = base_folder + 'test_products.sdf'\n",
    "# train\n",
    "train_r = Chem.ForwardSDMolSupplier(train_r_file, removeHs=False, sanitize=False)\n",
    "train_r = [x for x in train_r]\n",
    "train_ts = Chem.ForwardSDMolSupplier(train_ts_file, removeHs=False, sanitize=False)\n",
    "train_ts = [x for x in train_ts]\n",
    "train_p = Chem.ForwardSDMolSupplier(train_p_file, removeHs=False, sanitize=False)\n",
    "train_p = [x for x in train_p]\n",
    "# test\n",
    "test_r = Chem.ForwardSDMolSupplier(test_r_file, removeHs=False, sanitize=False)\n",
    "test_r = [x for x in test_r]\n",
    "test_ts = Chem.ForwardSDMolSupplier(test_ts_file, removeHs=False, sanitize=False)\n",
    "test_ts = [x for x in test_ts]\n",
    "test_p = Chem.ForwardSDMolSupplier(test_p_file, removeHs=False, sanitize=False)\n",
    "test_p = [x for x in test_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionDatasetInstance(torch.utils.data.Dataset):\n",
    "    \"\"\"Creates instance of reaction dataset. Has functions for train and test sets.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_r_name, train_ts_name, train_p_name, \n",
    "                       test_r_name,  test_ts_name,  test_p_name, base_folder='data/'):\n",
    "        self.base_folder = base_folder\n",
    "        train_r_file = base_folder + train_r_name\n",
    "        train_ts_file = base_folder + train_ts_name\n",
    "        train_p_file = base_folder + train_p_name\n",
    "        test_r_file = base_folder + test_r_name\n",
    "        test_ts_file = base_folder + test_ts_name\n",
    "        test_p_file = base_folder + test_p_name\n",
    "        self.train_r, self.train_ts, self.train_p = create_training_set(train_r_file, train_ts_file, train_p_file)\n",
    "        self.test_r, self.test_ts, self.test_p = create_test_set(test_r_file, test_ts_file, test_p_file)\n",
    "\n",
    "    def create_training_set(self, train_r_file, train_ts_file, train_p_file):\n",
    "        train_r = sdf_to_rdmol(train_r_file)\n",
    "        train_ts = sdf_to_rdmol(train_ts_file)\n",
    "        train_p = sdf_to_rdmol(train_p_file)\n",
    "        return train_r, train_ts, train_p\n",
    "    \n",
    "    def create_test_set(self, test_r_file, test_ts_file, test_p_file):\n",
    "        test_r = sdf_to_rdmol(test_r_file)\n",
    "        test_ts = sdf_to_rdmol(test_ts_file)\n",
    "        test_p = sdf_to_rdmol(test_p_file)\n",
    "        return test_r, test_ts, test_p\n",
    "    \n",
    "    def sdf_to_rdmol(self, geometry_file):\n",
    "        geometries = Chem.ForwardSDMolSupplier(geometry_file, removeHs=False, sanitize=False)\n",
    "        geometries = [x for x in geometries]\n",
    "\n",
    "    def coordinate_to_interatomic_dist():\n",
    "        return\n",
    "\n",
    "    # visualisation funcs here, eda subclass?\n",
    "    #  compare averages\n",
    "\n",
    "    # need option to display as whole reaction (r, ts, p) for train and test\n",
    "        # perhaps can identify scaffold bias at some point on the split\n",
    "        # can visualise how the interatomic dynamics change\n",
    "    \n",
    "    # should have some way of comparing models formally? class, etc.?\n",
    "        # compare ts_gen TS: average, initial guess, final estimate, real.\n",
    "            # where is there most room for improvement\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### put R-TS-P into dataloader\n",
    "\n",
    "# really, it's train_r, train_p, val_ts then test_r, test_ts, test_p\n",
    "\n",
    "# zip rs and ps\n",
    "train_rxn_endpoints = list(zip(train_r, train_p))\n",
    "\n",
    "\n",
    "# class ReactionDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b7c0f232b247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# === forward ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# setting optimiser\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# training and testing the VAE\n",
    "epochs = 5\n",
    "codes = dict(mean=list(), log_var=list(), y=list())\n",
    "for epoch in range(0, epochs+1):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for real_TS, _ in train_loader:\n",
    "            real_TS = real_TS.to(device)\n",
    "            # === forward ===\n",
    "            generated_TS, mean_z, log_var_z = model(x)\n",
    "            loss = loss_func(generated_TS, real_TS, mean_z, log_var_z)\n",
    "            train_loss += loss.item()\n",
    "            # === backward ===\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # === log ===\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "        \n",
    "    # testing\n",
    "    means, log_vars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mean, log_var).item()\n",
    "            # === log ===\n",
    "            means.append(mean.detach())\n",
    "            log_vars.append(log_var.detach())\n",
    "            labels.append(y.detach())\n",
    "    # === log ===\n",
    "    codes['mean'].append(torch.cat(means))\n",
    "    codes['log_var'].append(torch.cat(log_vars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===> Test set loss: {test_loss:.4f}')\n",
    "    display_images(x, x_hat, 1, f'Epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # linear (size of input, 2d), size of input= max possible size i.e. largest mol\n",
    "            nn.Linear(input_size, d**2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, input_size)\n",
    "            # would use sigmoid here if input was between 0 and 1\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mean_z, log_var_z):\n",
    "        if self.training:\n",
    "            # eps = normal(loc=0, scale=1, size=(len(graphs.nodes), self.latent_dim=2d))\n",
    "            # since variances only positive, computing log allows you to output full real range for encoder\n",
    "            eps = normal(0, 1, size=(len(input_nodes), latent_dims=2d))\n",
    "            z = mean_z + eps * sqrt(exp(log_var_z))\n",
    "            return z\n",
    "        else:\n",
    "            return mean_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape input into a vector, then reshape using view(-1, batchsize=2, d)\n",
    "        params_z = self.encoder(x.view(-1, input_size)).view(-1, 2, d)\n",
    "        \n",
    "        mean_z = params_z[:, 0, :]\n",
    "        log_var_z = params_z[:, 1, :]\n",
    "        z = self.reparameterise(mean_z, log_var_z)\n",
    "        return self.decoder(z), mean_z, log_var_z \n",
    "\n",
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting optimiser\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# reconstruction + KL divergence losses summed over all elements\n",
    "def loss_function(x_hat, x, mean_z, log_var_z, beta):\n",
    "    # binary cross entropy between input and reconstruction\n",
    "    BCE = nn.functional.binary_cross_entropy(x_hat, x.view(-1, 784), reduction='sum') \n",
    "    # kl divergence: var is linear, - log var is logarithmic, mean is squared \n",
    "    KLD = 0.5 * torch.sum(exp(log_var_z) - log_var_z - 1 + mean_z**2)\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing the VAE\n",
    "epochs = 5\n",
    "codes = dict(mean=list(), log_var=list(), y=list())\n",
    "for epoch in range(0, epochs+1):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x_hat, x, mean, log_var)\n",
    "            train_loss += loss.item()\n",
    "            # === backward ===\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # === log ===\n",
    "        print(f'====> Epoch: {epoch} Average loss: {\n",
    "            train_loss / len(train_loader.dataset):.4f}')\n",
    "        \n",
    "    # testing\n",
    "    means, log_vars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mean, log_var).item()\n",
    "            # === log ===\n",
    "            means.append(mean.detach())\n",
    "            log_vars.append(log_var.detach())\n",
    "            labels.append(y.detach())\n",
    "    # === log ===\n",
    "    codes['mean'].append(torch.cat(means))\n",
    "    codes['log_var'].append(torch.cat(log_vars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===> Test set loss: {test_loss:.4f}')\n",
    "    display_images(x, x_hat, 1, f'Epoch {epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a few samples\n",
    "N = 16\n",
    "z = torch.randn((N, d)).to(device)\n",
    "sample = model.decoder(z)\n",
    "display_images(None, sample, N//4, count=True)\n",
    "\n",
    "# Choose starting and ending point for the interpolation -> shows original and reconstructed\n",
    "\n",
    "A, B = 1, 14\n",
    "sample = model.decoder(torch.stack((mean[A].data, mean[B].data), 0))\n",
    "display_images(None, torch.stack(((\n",
    "    x[A].data.view(-1),\n",
    "    x[B].data.view(-1),\n",
    "    sample.data[0],\n",
    "    sample.data[1]\n",
    ")), 0))"
   ]
  }
 ]
}