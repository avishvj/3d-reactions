{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from numpy import exp, sqrt\n",
    "from numpy.random import normal\n",
    "from torch import exp, sqrt, randn_like\n",
    "from rdkit import Chem\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 4      # arbitrary\n",
    "MAX_SIZE = 22*3     # try first with (x,y,z) for each atom\n",
    "\n",
    "class MoleculeVAE(nn.Module):\n",
    "    def __init__(self): # maybe also init encoder, decoder, in_channels, out_channels, depth, hidden size, dropout, gnn_type\n",
    "        # self.decoder = DefaultDecoder() if decoder is None else decoder\n",
    "        # maybe pass in data as reaction frame here too?\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(MAX_SIZE, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2*LATENT_DIM)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2*LATENT_DIM, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, MAX_SIZE)\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            if self.gnn_type == ?:\n",
    "                self.convs.append(e.g. GCNConv(args))\n",
    "        \"\"\"\n",
    "    \n",
    "    def reparameterise(self, mean_z, log_var_z):\n",
    "        if self.training:\n",
    "            eps = randn_like() # is this meant to be randn_like(log_var_z)?\n",
    "            z = mean_z + eps * sqrt(exp(log_var_z))\n",
    "            return z\n",
    "        else:\n",
    "            return mean_z\n",
    "    \n",
    "    def forward(self, x): # note: might be easier to do this the other way explicitly defining mean and var\n",
    "        # reshape input into a vector, then reshape using view(-1, batchsize=2, d)\n",
    "        params_z = self.encoder(x.view(-1, input_size)).view(-1, 2, LATENT_DIM) # this encoder may need to be changed\n",
    "        mean_z = params_z[:, 0, :]\n",
    "        log_var_z = params_z[:, 1, :]\n",
    "        z = self.reparameterise(mean_z, log_var_z)\n",
    "        return self.decoder(z), mean_z, log_var_z \n",
    "\n",
    "    # x_hat = model TS; x = TS\n",
    "    def loss_func(generated_TS, real_TS, mean_z, log_var_z, beta=1):\n",
    "        BCE = nn.functional.binary_cross_entropy(generated_TS, real_TS.view(-1, MAX_SIZE), reduction='sum') \n",
    "        KLD = 0.5 * torch.sum(exp(log_var_z) - log_var_z - 1 + mean_z**2)\n",
    "        return BCE + beta * KLD\n",
    "\n",
    "# define device and model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MoleculeVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/6739 [00:00<?, ?it/s]Processing...\n",
      "100%|██████████| 6739/6739 [00:06<00:00, 1093.34it/s]\n",
      "100%|██████████| 6739/6739 [00:06<00:00, 976.26it/s]\n",
      "100%|██████████| 6739/6739 [00:06<00:00, 982.86it/s] \n",
      "100%|██████████| 842/842 [00:00<00:00, 1047.57it/s]\n",
      "100%|██████████| 842/842 [00:00<00:00, 892.46it/s]\n",
      "100%|██████████| 842/842 [00:00<00:00, 1050.84it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "rxn_dataset = ReactionDataset(r'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "ReactionDataset(22743)"
      ]
     },
     "metadata": {},
     "execution_count": 255
    }
   ],
   "source": [
    "rxn_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import HybridizationType\n",
    "from rdkit.Chem.rdchem import BondType as BT\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ReactionDataset(InMemoryDataset):\n",
    "    \"\"\" Creates instance of reaction dataset. \"\"\"\n",
    "\n",
    "    types = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\n",
    "    bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\n",
    "\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(ReactionDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['/raw/train_reactants.sdf', '/raw/train_ts.sdf', '/raw/train_products.sdf', '/raw/test_reactants.sdf', '/raw/test_ts.sdf', '/raw/test_products.sdf']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        \"\"\" If files already in processed folder, this processing is skipped. \"\"\"\n",
    "        return ['train_r.pt', 'train_ts.pt', 'train_p.pt', 'test_r.pt', 'test_ts.pt', 'test_p.pt']\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\" Not required in this project. \"\"\"\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Processes each of the six geometry files and appends to a list. \n",
    "            Code mostly lifted from QM9 dataset creation https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/qm9.html \n",
    "        \"\"\"\n",
    "\n",
    "        for g_idx, geometry_file in enumerate(self.raw_file_names): # should maybe create enum with raw-processed together\n",
    "            \n",
    "            data_list = []\n",
    "            full_path = self.root + geometry_file\n",
    "            geometries = Chem.SDMolSupplier(full_path, removeHs=False, sanitize=False)\n",
    "            \n",
    "            for i, mol in enumerate(tqdm(geometries)):\n",
    "                \n",
    "                N = mol.GetNumAtoms()\n",
    "\n",
    "                # get atom positions as matrix w shape [num_nodes, num_dimensions] = [num_atoms, 3]\n",
    "                atom_data = geometries.GetItemText(i).split('\\n')[4:4 + N] \n",
    "                atom_positions = [[float(x) for x in line.split()[:3]] for line in atom_data]\n",
    "                atom_positions = torch.tensor(atom_positions, dtype=torch.float)\n",
    "\n",
    "                # all the features\n",
    "                type_idx = []\n",
    "                atomic_number = []\n",
    "                aromatic = []\n",
    "                sp = []\n",
    "                sp2 = []\n",
    "                sp3 = []\n",
    "                num_hs = []\n",
    "\n",
    "                # atom/node features\n",
    "                for atom in mol.GetAtoms():\n",
    "                    type_idx.append(self.types[atom.GetSymbol()])\n",
    "                    atomic_number.append(atom.GetAtomicNum())\n",
    "                    aromatic.append(1 if atom.GetIsAromatic() else 0)\n",
    "                    hybridisation = atom.GetHybridization()\n",
    "                    sp.append(1 if hybridisation == HybridizationType.SP else 0)\n",
    "                    sp2.append(1 if hybridisation == HybridizationType.SP2 else 0)\n",
    "                    sp3.append(1 if hybridisation == HybridizationType.SP3 else 0)\n",
    "\n",
    "                # bond/edge features\n",
    "                row, col, edge_type = [], [], []\n",
    "                for bond in mol.GetBonds(): \n",
    "                    start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "                    row += [start, end]\n",
    "                    col += [end, start]\n",
    "                    # edge type for each bond type; *2 because both ways\n",
    "                    edge_type += 2 * [self.bonds[bond.GetBondType()]]\n",
    "\n",
    "                # edge_index is graph connectivity in COO format with shape [2, num_edges]\n",
    "                edge_index = torch.tensor([row, col], dtype=torch.long) \n",
    "                edge_type = torch.tensor(edge_type, dtype=torch.long)\n",
    "                # one hot the edge types into distinct types for bonds\n",
    "                # edge_attr is edge feature matrix with shape [num_edges, num_edge_features]\n",
    "                edge_attr = F.one_hot(edge_type, num_classes=len(self.bonds)).to(torch.float) \n",
    "\n",
    "                # order edges based on combined ascending order\n",
    "                perm = (edge_index[0] * N + edge_index[1]).argsort()\n",
    "                edge_index = edge_index[:, perm]\n",
    "                edge_type = edge_type[perm]\n",
    "                edge_attr = edge_attr[perm]\n",
    "\n",
    "                row, col = edge_index\n",
    "                z = torch.tensor(atomic_number, dtype=torch.long)\n",
    "                hs = (z == 1).to(torch.float) # hydrogens\n",
    "                # https://abderhasan.medium.com/pytorchs-scatter-function-a-visual-explanation-351d25c05c73\n",
    "                # helps with one-hot encoding, should come back to this\n",
    "                num_hs = scatter(hs[row], col, dim_size=N).tolist() \n",
    "                \n",
    "                x1 = F.one_hot(torch.tensor(type_idx), num_classes=len(self.types))\n",
    "                x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, num_hs], dtype=torch.float).t().contiguous()\n",
    "                x = torch.cat([x1.to(torch.float), x2], dim=-1)\n",
    "\n",
    "                # no direct y since plan to decode to TS\n",
    "                data = Data(x=x, z=z, pos=atom_positions, edge_index=edge_index, edge_attr=edge_attr, idx=i)\n",
    "\n",
    "                data_list.append(data)\n",
    "\n",
    "                # if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                #     continue\n",
    "                # if self.pre_transform is not None:\n",
    "                #     data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(self.collate(data_list), self.processed_paths[g_idx]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReactionDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Creates instance of reaction dataset. Has functions for train and test sets.\"\"\"\n",
    "    \n",
    "    def __init__(self, train_r_name, train_ts_name, train_p_name, \n",
    "                       test_r_name,  test_ts_name,  test_p_name, base_folder='data/'):\n",
    "        self.base_folder = base_folder\n",
    "        train_r_file = base_folder + train_r_name\n",
    "        train_ts_file = base_folder + train_ts_name\n",
    "        train_p_file = base_folder + train_p_name\n",
    "        test_r_file = base_folder + test_r_name\n",
    "        test_ts_file = base_folder + test_ts_name\n",
    "        test_p_file = base_folder + test_p_name\n",
    "        self.train_r, self.train_ts, self.train_p = create_training_set(train_r_file, train_ts_file, train_p_file)\n",
    "        self.test_r, self.test_ts, self.test_p = create_test_set(test_r_file, test_ts_file, test_p_file)\n",
    "\n",
    "        # sdf to rdmol to PTG data\n",
    "\n",
    "    def create_training_set(self, train_r_file, train_ts_file, train_p_file):\n",
    "        train_r = sdf_to_rdmol(train_r_file)\n",
    "        train_ts = sdf_to_rdmol(train_ts_file)\n",
    "        train_p = sdf_to_rdmol(train_p_file)\n",
    "        return train_r, train_ts, train_p\n",
    "    \n",
    "    def create_test_set(self, test_r_file, test_ts_file, test_p_file):\n",
    "        test_r = sdf_to_rdmol(test_r_file)\n",
    "        test_ts = sdf_to_rdmol(test_ts_file)\n",
    "        test_p = sdf_to_rdmol(test_p_file)\n",
    "        return test_r, test_ts, test_p\n",
    "    \n",
    "    def sdf_to_rdmol(self, geometry_file):\n",
    "        geometries = Chem.ForwardSDMolSupplier(geometry_file, removeHs=False, sanitize=False)\n",
    "        geometries = [x for x in geometries]\n",
    "    \n",
    "    def process_rdmol(self, geometry_file):\n",
    "        geometries = Chem.SDMolSupplier(geometry_file, removeHs=False, sanitize=False)\n",
    "\n",
    "\n",
    "    def coordinate_to_interatomic_dist():\n",
    "        # maybe generalise this function with a flag for different initial inputs\n",
    "            # e.g. interatomic distances, Z-matrix, nuclear charge, etc.\n",
    "        # if these are different enough, may be better to have this instance as an abstract class then have implementations for each matrix type\n",
    "        return\n",
    "\n",
    "    def visualise_feature_dynamics()\n",
    "        # may need a function here to calculate reaction centre\n",
    "        # visualise how features change over time e.g. interatomic distances\n",
    "        # how much are interatomic distances changing? what precision of model do we need to capture these differences?\n",
    "        return\n",
    "\n",
    "    def dataset_properties():\n",
    "        # functions for rdkit molecule properties to compare\n",
    "        return\n",
    "\n",
    "    # other funcs: scaffold bias on train-test split - how would that work in 3D?\n",
    "\n",
    "class ModelRun():\n",
    "    def __init__(self, training_rxns, test_rxns, model):\n",
    "        self.training_rxns = training_rxns\n",
    "        self.test_rxns = test_rxns\n",
    "\n",
    "    # plot loss \n",
    "    # plotting evaluation here\n",
    "    \n",
    "    def preprocess_data():\n",
    "        # preprocess the data for each model as it suits\n",
    "\n",
    "        return\n",
    "    \n",
    "    # compare model TS: average, initial guesses, final estimates, reals\n",
    "        # will allow me to compare several models against each other\n",
    "    \n",
    "    # will have funcs to evaluate structure of output\n",
    "        # for generative models: evaluate structure of latent space\n",
    "        # evaluate how R and P are combined\n",
    "\n",
    "    # how are TS formed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### put R-TS-P into dataloader\n",
    "\n",
    "# really, it's train_r, train_p, val_ts then test_r, test_ts, test_p\n",
    "\n",
    "# zip rs and ps\n",
    "train_rxn_endpoints = list(zip(train_r, train_p))\n",
    "\n",
    "\n",
    "# class ReactionDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "def create_dataloader(args):\n",
    "\n",
    "    if isinstance(modes, str):\n",
    "        modes = [modes]\n",
    "\n",
    "\n",
    "def construct_loader(args, modes=('train', 'val')):\n",
    "\n",
    "    if isinstance(modes, str):\n",
    "        modes = [modes]\n",
    "\n",
    "    data_df = pd.read_csv(args.data_path)\n",
    "\n",
    "    smiles = data_df.iloc[:, 0].values\n",
    "    labels = data_df.iloc[:, 1].values.astype(np.float32)\n",
    "\n",
    "    loaders = []\n",
    "    for mode in modes:\n",
    "        dataset = MolDataset(smiles, labels, args, mode)\n",
    "        loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=not args.no_shuffle if mode == 'train' else False,\n",
    "                            num_workers=args.num_workers,\n",
    "                            pin_memory=True,\n",
    "                            sampler=StereoSampler(dataset) if args.shuffle_pairs else None)\n",
    "        loaders.append(loader)\n",
    "\n",
    "    if len(loaders) == 1:\n",
    "        return loaders[0]\n",
    "    else:\n",
    "        return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b7c0f232b247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# === forward ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# setting optimiser\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# training and testing the VAE\n",
    "epochs = 5\n",
    "codes = dict(mean=list(), log_var=list(), y=list())\n",
    "for epoch in range(0, epochs+1):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for real_TS, _ in train_loader:\n",
    "            real_TS = real_TS.to(device)\n",
    "            # === forward ===\n",
    "            generated_TS, mean_z, log_var_z = model(x)\n",
    "            loss = loss_func(generated_TS, real_TS, mean_z, log_var_z)\n",
    "            train_loss += loss.item()\n",
    "            # === backward ===\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # === log ===\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "        \n",
    "    # testing\n",
    "    means, log_vars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mean, log_var).item()\n",
    "            # === log ===\n",
    "            means.append(mean.detach())\n",
    "            log_vars.append(log_var.detach())\n",
    "            labels.append(y.detach())\n",
    "    # === log ===\n",
    "    codes['mean'].append(torch.cat(means))\n",
    "    codes['log_var'].append(torch.cat(log_vars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===> Test set loss: {test_loss:.4f}')\n",
    "    display_images(x, x_hat, 1, f'Epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # linear (size of input, 2d), size of input= max possible size i.e. largest mol\n",
    "            nn.Linear(input_size, d**2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, input_size)\n",
    "            # would use sigmoid here if input was between 0 and 1\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mean_z, log_var_z):\n",
    "        if self.training:\n",
    "            # eps = normal(loc=0, scale=1, size=(len(graphs.nodes), self.latent_dim=2d))\n",
    "            # since variances only positive, computing log allows you to output full real range for encoder\n",
    "            eps = normal(0, 1, size=(len(input_nodes), latent_dims=2d))\n",
    "            z = mean_z + eps * sqrt(exp(log_var_z))\n",
    "            return z\n",
    "        else:\n",
    "            return mean_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape input into a vector, then reshape using view(-1, batchsize=2, d)\n",
    "        params_z = self.encoder(x.view(-1, input_size)).view(-1, 2, d)\n",
    "        \n",
    "        mean_z = params_z[:, 0, :]\n",
    "        log_var_z = params_z[:, 1, :]\n",
    "        z = self.reparameterise(mean_z, log_var_z)\n",
    "        return self.decoder(z), mean_z, log_var_z \n",
    "\n",
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting optimiser\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# reconstruction + KL divergence losses summed over all elements\n",
    "def loss_function(x_hat, x, mean_z, log_var_z, beta):\n",
    "    # binary cross entropy between input and reconstruction\n",
    "    BCE = nn.functional.binary_cross_entropy(x_hat, x.view(-1, 784), reduction='sum') \n",
    "    # kl divergence: var is linear, - log var is logarithmic, mean is squared \n",
    "    KLD = 0.5 * torch.sum(exp(log_var_z) - log_var_z - 1 + mean_z**2)\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing the VAE\n",
    "epochs = 5\n",
    "codes = dict(mean=list(), log_var=list(), y=list())\n",
    "for epoch in range(0, epochs+1):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x_hat, x, mean, log_var)\n",
    "            train_loss += loss.item()\n",
    "            # === backward ===\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # === log ===\n",
    "        print(f'====> Epoch: {epoch} Average loss: {\n",
    "            train_loss / len(train_loader.dataset):.4f}')\n",
    "        \n",
    "    # testing\n",
    "    means, log_vars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mean, log_var).item()\n",
    "            # === log ===\n",
    "            means.append(mean.detach())\n",
    "            log_vars.append(log_var.detach())\n",
    "            labels.append(y.detach())\n",
    "    # === log ===\n",
    "    codes['mean'].append(torch.cat(means))\n",
    "    codes['log_var'].append(torch.cat(log_vars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===> Test set loss: {test_loss:.4f}')\n",
    "    display_images(x, x_hat, 1, f'Epoch {epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a few samples\n",
    "N = 16\n",
    "z = torch.randn((N, d)).to(device)\n",
    "sample = model.decoder(z)\n",
    "display_images(None, sample, N//4, count=True)\n",
    "\n",
    "# Choose starting and ending point for the interpolation -> shows original and reconstructed\n",
    "\n",
    "A, B = 1, 14\n",
    "sample = model.decoder(torch.stack((mean[A].data, mean[B].data), 0))\n",
    "display_images(None, torch.stack(((\n",
    "    x[A].data.view(-1),\n",
    "    x[B].data.view(-1),\n",
    "    sample.data[0],\n",
    "    sample.data[1]\n",
    ")), 0))"
   ]
  }
 ]
}