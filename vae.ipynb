{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3",
   "display_name": "Python 3.6.13 64-bit (conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from numpy import exp, sqrt\n",
    "from numpy.random import normal\n",
    "from torch import exp, sqrt, randn_like\n",
    "from rdkit import Chem\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic GCNConv\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, geometry_dataset: InMemoryDataset, hidden: List[int] = 2, dropout: float = 0.5):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        num_features = [geometry_dataset.data.x.shape[1]] + hidden + [geometry_dataset.num_classes]\n",
    "        layers = []\n",
    "        for in_features, out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "test_r_data.num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/rusty1s/pytorch_geometric/blob/master/examples/autoencoder.py\n",
    "def train():\n",
    "    # train\n",
    "    # use optimiser\n",
    "    # encode\n",
    "    # decode to TS\n",
    "    # evaluate loss compared to expected TS\n",
    "    # loss.backward() \n",
    "    # optimiser.step()\n",
    "    # return float(loss)\n",
    "\n",
    "# run test_r and test_p through encoder and decode to TS, eval against test_ts\n",
    "def test():\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = 4      # arbitrary\n",
    "MAX_SIZE = 22*3     # try first with (x,y,z) for each atom\n",
    "\n",
    "class MoleculeVAE(nn.Module):\n",
    "    def __init__(self): # maybe also init encoder, decoder, in_channels, out_channels, depth, hidden size, dropout, gnn_type\n",
    "        # self.decoder = DefaultDecoder() if decoder is None else decoder\n",
    "        # maybe pass in data as reaction frame here too?\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(MAX_SIZE, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2*LATENT_DIM)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2*LATENT_DIM, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, MAX_SIZE)\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        for _ in range(self.depth):\n",
    "            if self.gnn_type == ?:\n",
    "                self.convs.append(e.g. GCNConv(args))\n",
    "        \"\"\"\n",
    "    \n",
    "    def reparameterise(self, mean_z, log_var_z):\n",
    "        if self.training:\n",
    "            eps = randn_like() # is this meant to be randn_like(log_var_z)?\n",
    "            z = mean_z + eps * sqrt(exp(log_var_z))\n",
    "            return z\n",
    "        else:\n",
    "            return mean_z\n",
    "    \n",
    "    def forward(self, x): # note: might be easier to do this the other way explicitly defining mean and var\n",
    "        # reshape input into a vector, then reshape using view(-1, batchsize=2, d)\n",
    "        params_z = self.encoder(x.view(-1, input_size)).view(-1, 2, LATENT_DIM) # this encoder may need to be changed\n",
    "        mean_z = params_z[:, 0, :]\n",
    "        log_var_z = params_z[:, 1, :]\n",
    "        z = self.reparameterise(mean_z, log_var_z)\n",
    "        return self.decoder(z), mean_z, log_var_z \n",
    "\n",
    "    # x_hat = model TS; x = TS\n",
    "    def loss_func(generated_TS, real_TS, mean_z, log_var_z, beta=1):\n",
    "        BCE = nn.functional.binary_cross_entropy(generated_TS, real_TS.view(-1, MAX_SIZE), reduction='sum') \n",
    "        KLD = 0.5 * torch.sum(exp(log_var_z) - log_var_z - 1 + mean_z**2)\n",
    "        return BCE + beta * KLD\n",
    "\n",
    "# define device and model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MoleculeVAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_vae.data_processors.grambow_processor import ReactionDataset\n",
    "\n",
    "base_path = r'data/'\n",
    "\n",
    "train_r_data = ReactionDataset(base_path, geo_file = 'train_r') \n",
    "train_ts_data = ReactionDataset(base_path, geo_file = 'train_ts')\n",
    "train_p_data = ReactionDataset(base_path, geo_file = 'train_p')  \n",
    "\n",
    "test_r_data = ReactionDataset(base_path, geo_file = 'test_r') \n",
    "test_ts_data = ReactionDataset(base_path, geo_file = 'test_ts')\n",
    "test_p_data = ReactionDataset(base_path, geo_file = 'test_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# either take in reaction dataset or take in .sdf?\n",
    "# might be better to keep these functions inside the original processor\n",
    "class EDA():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # TODO\n",
    "    def visualise_feature_dynamics(self):\n",
    "        # takes in reactants, ts, products; calculates reaction centre\n",
    "        # compare how reaction centre changes: by how much, what precision do we need?\n",
    "        # could do similar with interatomic distances\n",
    "        return\n",
    "\n",
    "# model run class\n",
    "class ModelRun():\n",
    "    def __init__(self, training_rxns, test_rxns, model):\n",
    "        self.training_rxns = training_rxns\n",
    "        self.test_rxns = test_rxns\n",
    "\n",
    "    # plot loss \n",
    "    # plotting evaluation here\n",
    "    \n",
    "    def preprocess_data():\n",
    "        # preprocess the data for each model as it suits\n",
    "        return\n",
    "    \n",
    "    # compare model TS: average, initial guesses, final estimates, reals\n",
    "        # will allow me to compare several models against each other\n",
    "    \n",
    "    # will have funcs to evaluate structure of output\n",
    "        # for generative models: evaluate structure of latent space\n",
    "        # evaluate how R and P are combined\n",
    "\n",
    "    # how are TS formed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "def create_dataloader(args):\n",
    "\n",
    "    if isinstance(modes, str):\n",
    "        modes = [modes]\n",
    "\n",
    "\n",
    "def construct_loader(args, modes=('train', 'val')):\n",
    "\n",
    "    if isinstance(modes, str):\n",
    "        modes = [modes]\n",
    "\n",
    "    data_df = pd.read_csv(args.data_path)\n",
    "\n",
    "    smiles = data_df.iloc[:, 0].values\n",
    "    labels = data_df.iloc[:, 1].values.astype(np.float32)\n",
    "\n",
    "    loaders = []\n",
    "    for mode in modes:\n",
    "        dataset = MolDataset(smiles, labels, args, mode)\n",
    "        loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=not args.no_shuffle if mode == 'train' else False,\n",
    "                            num_workers=args.num_workers,\n",
    "                            pin_memory=True,\n",
    "                            sampler=StereoSampler(dataset) if args.shuffle_pairs else None)\n",
    "        loaders.append(loader)\n",
    "\n",
    "    if len(loaders) == 1:\n",
    "        return loaders[0]\n",
    "    else:\n",
    "        return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'test_loader' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b7c0f232b247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mtest_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# === forward ===\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# setting optimiser\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# training and testing the VAE\n",
    "epochs = 5\n",
    "codes = dict(mean=list(), log_var=list(), y=list())\n",
    "for epoch in range(0, epochs+1):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for real_TS, _ in train_loader:\n",
    "            real_TS = real_TS.to(device)\n",
    "            # === forward ===\n",
    "            generated_TS, mean_z, log_var_z = model(x)\n",
    "            loss = loss_func(generated_TS, real_TS, mean_z, log_var_z)\n",
    "            train_loss += loss.item()\n",
    "            # === backward ===\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # === log ===\n",
    "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
    "        \n",
    "    # testing\n",
    "    means, log_vars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mean, log_var).item()\n",
    "            # === log ===\n",
    "            means.append(mean.detach())\n",
    "            log_vars.append(log_var.detach())\n",
    "            labels.append(y.detach())\n",
    "    # === log ===\n",
    "    codes['mean'].append(torch.cat(means))\n",
    "    codes['log_var'].append(torch.cat(log_vars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===> Test set loss: {test_loss:.4f}')\n",
    "    display_images(x, x_hat, 1, f'Epoch {epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            # linear (size of input, 2d), size of input= max possible size i.e. largest mol\n",
    "            nn.Linear(input_size, d**2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, d * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(d, d ** 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d ** 2, input_size)\n",
    "            # would use sigmoid here if input was between 0 and 1\n",
    "        )\n",
    "\n",
    "    def reparameterise(self, mean_z, log_var_z):\n",
    "        if self.training:\n",
    "            # eps = normal(loc=0, scale=1, size=(len(graphs.nodes), self.latent_dim=2d))\n",
    "            # since variances only positive, computing log allows you to output full real range for encoder\n",
    "            eps = normal(0, 1, size=(len(input_nodes), latent_dims=2d))\n",
    "            z = mean_z + eps * sqrt(exp(log_var_z))\n",
    "            return z\n",
    "        else:\n",
    "            return mean_z\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reshape input into a vector, then reshape using view(-1, batchsize=2, d)\n",
    "        params_z = self.encoder(x.view(-1, input_size)).view(-1, 2, d)\n",
    "        \n",
    "        mean_z = params_z[:, 0, :]\n",
    "        log_var_z = params_z[:, 1, :]\n",
    "        z = self.reparameterise(mean_z, log_var_z)\n",
    "        return self.decoder(z), mean_z, log_var_z \n",
    "\n",
    "model = VAE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting optimiser\n",
    "learning_rate = 1e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# reconstruction + KL divergence losses summed over all elements\n",
    "def loss_function(x_hat, x, mean_z, log_var_z, beta):\n",
    "    # binary cross entropy between input and reconstruction\n",
    "    BCE = nn.functional.binary_cross_entropy(x_hat, x.view(-1, 784), reduction='sum') \n",
    "    # kl divergence: var is linear, - log var is logarithmic, mean is squared \n",
    "    KLD = 0.5 * torch.sum(exp(log_var_z) - log_var_z - 1 + mean_z**2)\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing the VAE\n",
    "epochs = 5\n",
    "codes = dict(mean=list(), log_var=list(), y=list())\n",
    "for epoch in range(0, epochs+1):\n",
    "    # training\n",
    "    if epoch > 0:\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x_hat, x, mean, log_var)\n",
    "            train_loss += loss.item()\n",
    "            # === backward ===\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "        # === log ===\n",
    "        print(f'====> Epoch: {epoch} Average loss: {\n",
    "            train_loss / len(train_loader.dataset):.4f}')\n",
    "        \n",
    "    # testing\n",
    "    means, log_vars, labels = list(), list(), list()\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # === forward ===\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            test_loss += loss_function(x_hat, x, mean, log_var).item()\n",
    "            # === log ===\n",
    "            means.append(mean.detach())\n",
    "            log_vars.append(log_var.detach())\n",
    "            labels.append(y.detach())\n",
    "    # === log ===\n",
    "    codes['mean'].append(torch.cat(means))\n",
    "    codes['log_var'].append(torch.cat(log_vars))\n",
    "    codes['y'].append(torch.cat(labels))\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===> Test set loss: {test_loss:.4f}')\n",
    "    display_images(x, x_hat, 1, f'Epoch {epoch}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating a few samples\n",
    "N = 16\n",
    "z = torch.randn((N, d)).to(device)\n",
    "sample = model.decoder(z)\n",
    "display_images(None, sample, N//4, count=True)\n",
    "\n",
    "# Choose starting and ending point for the interpolation -> shows original and reconstructed\n",
    "\n",
    "A, B = 1, 14\n",
    "sample = model.decoder(torch.stack((mean[A].data, mean[B].data), 0))\n",
    "display_images(None, torch.stack(((\n",
    "    x[A].data.view(-1),\n",
    "    x[B].data.view(-1),\n",
    "    sample.data[0],\n",
    "    sample.data[1]\n",
    ")), 0))"
   ]
  }
 ]
}