{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go over notes and build up from simpler models\r\n",
    "\r\n",
    "1. R AE \r\n",
    "2. R-P AE \r\n",
    "3. R encoder and TS decoder \r\n",
    "4. R-P encoder, TS decoder\r\n",
    "\r\n",
    "TODO:\r\n",
    "- Have an visualise function where you can plot embeddings e.g. umap, pca, tsne and interpolate between rs and ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch_geometric.nn import GCNConv, GAE\r\n",
    "from torch_geometric.utils import train_test_split_edges\r\n",
    "from ts_vae.data_processors.grambow_processor import ReactionDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Molecule Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(nn):\r\n",
    "    def _reset(item):\r\n",
    "        if hasattr(item, 'reset_parameters'):\r\n",
    "            item.reset_parameters()\r\n",
    "\r\n",
    "    if nn is not None:\r\n",
    "        if hasattr(nn, 'children') and len(list(nn.children())) > 0:\r\n",
    "            for item in nn.children():\r\n",
    "                _reset(item)\r\n",
    "        else:\r\n",
    "            _reset(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\r\n",
    "from torch_geometric.utils import (negative_sampling, remove_self_loops, add_self_loops)\r\n",
    "EPS = 1e-15\r\n",
    "\r\n",
    "class GAE(nn.Module):\r\n",
    "    def __init__(self, encoder, decoder = None):\r\n",
    "        super(GAE, self).__init__()\r\n",
    "        self.encoder = encoder\r\n",
    "        self.decoder = InnerProductDecoder()\r\n",
    "        GAE.reset_parameters(self)\r\n",
    "\r\n",
    "    def reset_parameters(self):\r\n",
    "        reset(self.encoder)\r\n",
    "        reset(self.decoder) \r\n",
    "\r\n",
    "    def encode(self, *args, **kwargs):\r\n",
    "        return self.encoder(*args, **kwargs)\r\n",
    "\r\n",
    "    def decode(self, *args, **kwargs):\r\n",
    "        return self.decoder(*args, **kwargs)\r\n",
    "\r\n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index = None):\r\n",
    "        \"\"\" BCE for input on its reconstruction. \"\"\"\r\n",
    "        pos_loss = -torch.log(self.decoder(z, pos_edge_index, sigmoid = True) + EPS).mean()\r\n",
    "\r\n",
    "        # no self-loops in negative samples\r\n",
    "        pos_edge_index, _ = remove_self_loops(pos_edge_index)\r\n",
    "        pos_edge_index, _ = add_self_loops(pos_edge_index)\r\n",
    "        if neg_edge_index is None:\r\n",
    "            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\r\n",
    "        neg_loss = -torch.log(1 - self.decoder(z, neg_edge_index, sigmoid = True) + EPS).mean()\r\n",
    "\r\n",
    "        return pos_loss + neg_loss\r\n",
    "\r\n",
    "    def test(self, z, pos_edge_index, neg_edge_index):\r\n",
    "        pos_y = z.new_ones(pos_edge_index.size(1))\r\n",
    "        neg_y = z.new_zeros(neg_edge_index.size(1))\r\n",
    "        y = torch.cat([pos_y, neg_y], dim=0)\r\n",
    "\r\n",
    "        pos_pred = self.decoder(z, pos_edge_index, sigmoid=True)\r\n",
    "        neg_pred = self.decoder(z, neg_edge_index, sigmoid=True)\r\n",
    "        pred = torch.cat([pos_pred, neg_pred], dim=0)\r\n",
    "\r\n",
    "        y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\r\n",
    "\r\n",
    "        return roc_auc_score(y, pred), average_precision_score(y, pred)\r\n",
    "\r\n",
    "class MolEncoder(nn.Module):\r\n",
    "    def __init__(self, in_channels, out_channels):\r\n",
    "        super(MolEncoder, self).__init__()\r\n",
    "        self.conv = GCNConv(in_channels, out_channels)\r\n",
    "\r\n",
    "    def forward(self, x, edge_index):\r\n",
    "        return self.conv(x, edge_index)\r\n",
    "\r\n",
    "class InnerProductDecoder(nn.Module):\r\n",
    "    def forward(self, z, edge_index, sigmoid = True):\r\n",
    "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim = 1)\r\n",
    "        return torch.sigmoid(value) if sigmoid else value\r\n",
    "    \r\n",
    "    def forward_all(self, z, sigmoid = True):\r\n",
    "        adj = torch.matmul(z, z.t())\r\n",
    "        return torch.sigmoid(adj) if sigmoid else adj\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gae(gae, opt, x, train_pos_edge_index):\r\n",
    "    gae.train()\r\n",
    "    opt.zero_grad()\r\n",
    "    z = gae.encode(x, train_pos_edge_index)\r\n",
    "    loss = gae.recon_loss(z, train_pos_edge_index)\r\n",
    "    loss.backward()\r\n",
    "    opt.step()\r\n",
    "    return float(loss)\r\n",
    "\r\n",
    "def test_gae(gae, x, train_pos_edge_index, test_pos_edge_index, test_neg_edge_index):\r\n",
    "    gae.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        z = gae.encode(x, train_pos_edge_index)\r\n",
    "    return gae.test(z, test_pos_edge_index, test_neg_edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reactant Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model data\r\n",
    "base_path = r'data/'\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.2\r\n",
    "\r\n",
    "# reactant data\r\n",
    "r_dataset = ReactionDataset(base_path, geo_file = 'train_r') \r\n",
    "r_data = r_dataset.data\r\n",
    "r_data.train_mask = r_data.val_mask = r_data.test_mask = r_data.y = None\r\n",
    "r_data = train_test_split_edges(data = r_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "r_x = r_data.x.to(device)\r\n",
    "r_train_pos_edge_index = r_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# reactant encoder\r\n",
    "r_num_node_fs = r_data.num_node_features\r\n",
    "r_latent_dim = 2\r\n",
    "r_ae = GAE(MolEncoder(r_num_node_fs, r_latent_dim))\r\n",
    "r_opt = torch.optim.Adam(r_ae.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, AUC: 0.8173, AP: 0.6778\n",
      "Epoch: 020, AUC: 0.8166, AP: 0.6770\n",
      "Epoch: 030, AUC: 0.8162, AP: 0.6764\n",
      "Epoch: 040, AUC: 0.7617, AP: 0.6447\n",
      "Epoch: 050, AUC: 0.6425, AP: 0.5646\n",
      "Epoch: 060, AUC: 0.5798, AP: 0.5157\n",
      "Epoch: 070, AUC: 0.5806, AP: 0.5166\n",
      "Epoch: 080, AUC: 0.5808, AP: 0.5169\n",
      "Epoch: 090, AUC: 0.6277, AP: 0.5545\n",
      "Epoch: 100, AUC: 0.6814, AP: 0.5949\n"
     ]
    }
   ],
   "source": [
    "r_ae.reset_parameters()\r\n",
    "\r\n",
    "epochs = 100\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    loss = train_gae(r_ae, r_opt, r_x, r_data.train_pos_edge_index)\r\n",
    "    auc, ap = test_gae(r_ae, r_x, r_data.train_pos_edge_index, r_data.test_pos_edge_index, r_data.test_neg_edge_index)\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model data\r\n",
    "base_path = r'data/'\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.2\r\n",
    "\r\n",
    "# product data\r\n",
    "p_dataset = ReactionDataset(base_path, geo_file = 'train_p') \r\n",
    "p_data = p_dataset.data\r\n",
    "p_data.train_mask = p_data.val_mask = p_data.test_mask = p_data.y = None\r\n",
    "p_data = train_test_split_edges(data = p_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "p_x = p_data.x.to(device)\r\n",
    "p_train_pos_edge_index = p_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# product encoder\r\n",
    "p_num_node_fs = p_data.num_node_features\r\n",
    "p_latent_dim = 2\r\n",
    "p_ae = GAE(MolEncoder(p_num_node_fs, p_latent_dim))\r\n",
    "p_opt = torch.optim.Adam(p_ae.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, AUC: 0.7871, AP: 0.6643\n",
      "Epoch: 020, AUC: 0.7860, AP: 0.6698\n",
      "Epoch: 030, AUC: 0.7876, AP: 0.6959\n",
      "Epoch: 040, AUC: 0.8174, AP: 0.7411\n",
      "Epoch: 050, AUC: 0.8711, AP: 0.7817\n",
      "Epoch: 060, AUC: 0.8898, AP: 0.7884\n",
      "Epoch: 070, AUC: 0.8851, AP: 0.7721\n",
      "Epoch: 080, AUC: 0.8831, AP: 0.7654\n",
      "Epoch: 090, AUC: 0.8754, AP: 0.7582\n",
      "Epoch: 100, AUC: 0.8780, AP: 0.7616\n"
     ]
    }
   ],
   "source": [
    "p_ae.reset_parameters()\r\n",
    "\r\n",
    "epochs = 100\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    loss = train_gae(p_ae, p_opt, p_x, p_data.train_pos_edge_index)\r\n",
    "    auc, ap = test_gae(p_ae, p_x, p_data.train_pos_edge_index, p_data.test_pos_edge_index, p_data.test_neg_edge_index)\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same AE architecture but with reactant and product data concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did this first by hackily by creating a train_rp.sdf file with first 50 reactants and first 50 products so that the PyG data class and associated functions were automatically preserved. \r\n",
    "\r\n",
    "Next step is to see if I can concatenate the individual train_r and train_p datasets and then train on them normally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model data\r\n",
    "base_path = r'data/'\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.2 # 50 reactants and 50 products. will end up training on 50 reactants + 30 products and testing on 20 products\r\n",
    "\r\n",
    "# rp data\r\n",
    "rp_dataset = ReactionDataset(base_path, geo_file = 'train_rp_50')\r\n",
    "rp_data = rp_dataset.data\r\n",
    "rp_data.train_mask = rp_data.val_mask = rp_data.test_mask = rp_data.y = None\r\n",
    "rp_data = train_test_split_edges(data = rp_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "rp_x = rp_data.x.to(device)\r\n",
    "rp_train_pos_edge_index = rp_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# rp autoencoder\r\n",
    "rp_num_node_fs = rp_data.num_node_features\r\n",
    "rp_latent_dim = 2\r\n",
    "rp_ae = GAE(MolEncoder(rp_num_node_fs, rp_latent_dim))\r\n",
    "rp_opt = torch.optim.Adam(rp_ae.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, AUC: 0.6097, AP: 0.5554\n",
      "Epoch: 020, AUC: 0.3769, AP: 0.4165\n",
      "Epoch: 030, AUC: 0.5066, AP: 0.4703\n",
      "Epoch: 040, AUC: 0.6515, AP: 0.5624\n",
      "Epoch: 050, AUC: 0.7327, AP: 0.6283\n",
      "Epoch: 060, AUC: 0.8057, AP: 0.6766\n",
      "Epoch: 070, AUC: 0.8086, AP: 0.6799\n",
      "Epoch: 080, AUC: 0.8160, AP: 0.6874\n",
      "Epoch: 090, AUC: 0.8062, AP: 0.6833\n",
      "Epoch: 100, AUC: 0.8261, AP: 0.6953\n"
     ]
    }
   ],
   "source": [
    "rp_ae.reset_parameters()\r\n",
    "\r\n",
    "epochs = 100\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    loss = train_gae(rp_ae, rp_opt, rp_x, rp_data.train_pos_edge_index)\r\n",
    "    auc, ap = test_gae(rp_ae, rp_x, rp_data.train_pos_edge_index, rp_data.test_pos_edge_index, rp_data.test_neg_edge_index)\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate individual files. \r\n",
    "\r\n",
    "The new datasets may need to follow PyT or PyG dataset conventions e.g. pass in to InMemoryDataset? \\\r\n",
    "All operations are done on rp_dataset.data which is of type torch_geometric.data.data.Data. So will need to pass into this. \r\n",
    "\r\n",
    "Key words for network: dual, double, siamese, twin networks.\r\n",
    "- Siamese/twin: same weights while working in tandem on two different input vectors to compute comparable output vectors.\r\n",
    "\r\n",
    "Key words for training: simultaneous training.\r\n",
    "\r\n",
    "TODO:\r\n",
    "- Create ConcatDataset or more similar class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r'data/'\r\n",
    "r_dataset = ReactionDataset(base_path, geo_file = 'train_r')\r\n",
    "p_dataset = ReactionDataset(base_path, geo_file = 'train_p')\r\n",
    "total_mols = len(r_dataset) + len(p_dataset)\r\n",
    "\r\n",
    "concat_rp_dataset = []\r\n",
    "for i in range(total_mols):\r\n",
    "    if i < 100:\r\n",
    "        concat_rp_dataset.append(r_dataset[i])\r\n",
    "    else:\r\n",
    "        concat_rp_dataset.append(p_dataset[i - 100])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataLoader\r\n",
    "# data_list = [Data(...), ..., Data(...)] # loader = DataLoader(data_list, batch_size=32)\r\n",
    "\r\n",
    "concat_rp_loader = DataLoader(concat_rp_dataset, batch_size = 5)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch_scatter import scatter\r\n",
    "from torch_geometric.data import InMemoryDataset, Data\r\n",
    "from rdkit import Chem\r\n",
    "from rdkit.Chem.rdchem import HybridizationType\r\n",
    "from rdkit.Chem.rdchem import BondType as BT\r\n",
    "from tqdm import tqdm\r\n",
    "from enum import Enum\r\n",
    "\r\n",
    "TEMP_MOLS_LIMIT = 100\r\n",
    "\r\n",
    "class ConcatReactionDataset(InMemoryDataset):\r\n",
    "    types = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\r\n",
    "    bonds = {BT.SINGLE: 0, BT.DOUBLE: 1, BT.TRIPLE: 2, BT.AROMATIC: 3}\r\n",
    "\r\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\r\n",
    "        super(ConcatReactionDataset, self).__init__(root, transform, pre_transform)\r\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\r\n",
    "\r\n",
    "    @property\r\n",
    "    def raw_file_names(self):\r\n",
    "        return ['/raw/train_reactants.sdf', '/raw/train_products.sdf']\r\n",
    "    \r\n",
    "    @property\r\n",
    "    def processed_file_names(self):\r\n",
    "        \"\"\" If files already in processed folder, this processing is skipped. \r\n",
    "            Convenient for accessing the individual processed files without having to recreate them each time. \r\n",
    "        \"\"\"\r\n",
    "        return ['train_concat_rp.pt']\r\n",
    "\r\n",
    "    def download(self):\r\n",
    "        \"\"\" Not required in this project. \"\"\"\r\n",
    "        pass\r\n",
    "\r\n",
    "    def process(self):\r\n",
    "        \"\"\" Processes each of the six geometry files and appends to a list. \r\n",
    "            Code mostly lifted from QM9 dataset creation https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/qm9.html \r\n",
    "            Transforms molecules to their atom features and adjacency lists.\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        limit = TEMP_MOLS_LIMIT\r\n",
    "\r\n",
    "        concat_data_list = []\r\n",
    "\r\n",
    "        for g_idx, geometry_file in enumerate(self.raw_file_names): \r\n",
    "            \r\n",
    "            # data_list = [] # data_list for each geometry file\r\n",
    "            full_path = self.root + geometry_file\r\n",
    "            geometries = Chem.SDMolSupplier(full_path, removeHs=False, sanitize=False)\r\n",
    "\r\n",
    "            # get atom and edge features for each geometry\r\n",
    "            for i, mol in enumerate(tqdm(geometries)):\r\n",
    "\r\n",
    "                # temp soln cos of split edge memory issues\r\n",
    "                if i == limit:\r\n",
    "                    break\r\n",
    "                \r\n",
    "                N = mol.GetNumAtoms()\r\n",
    "                # get atom positions as matrix w shape [num_nodes, num_dimensions] = [num_atoms, 3]\r\n",
    "                atom_data = geometries.GetItemText(i).split('\\n')[4:4 + N] \r\n",
    "                atom_positions = [[float(x) for x in line.split()[:3]] for line in atom_data]\r\n",
    "                atom_positions = torch.tensor(atom_positions, dtype=torch.float)\r\n",
    "                # all the features\r\n",
    "                type_idx = []\r\n",
    "                atomic_number = []\r\n",
    "                aromatic = []\r\n",
    "                sp = []\r\n",
    "                sp2 = []\r\n",
    "                sp3 = []\r\n",
    "                num_hs = []\r\n",
    "\r\n",
    "                # atom/node features\r\n",
    "                for atom in mol.GetAtoms():\r\n",
    "                    type_idx.append(self.types[atom.GetSymbol()])\r\n",
    "                    atomic_number.append(atom.GetAtomicNum())\r\n",
    "                    aromatic.append(1 if atom.GetIsAromatic() else 0)\r\n",
    "                    hybridisation = atom.GetHybridization()\r\n",
    "                    sp.append(1 if hybridisation == HybridizationType.SP else 0)\r\n",
    "                    sp2.append(1 if hybridisation == HybridizationType.SP2 else 0)\r\n",
    "                    sp3.append(1 if hybridisation == HybridizationType.SP3 else 0)\r\n",
    "                    # !!! should do the features that lucky does: whether bonded, 3d_rbf\r\n",
    "\r\n",
    "                # bond/edge features\r\n",
    "                row, col, edge_type = [], [], []\r\n",
    "                for bond in mol.GetBonds(): \r\n",
    "                    start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\r\n",
    "                    row += [start, end]\r\n",
    "                    col += [end, start]\r\n",
    "                    # edge type for each bond type; *2 because both ways\r\n",
    "                    edge_type += 2 * [self.bonds[bond.GetBondType()]]\r\n",
    "                # edge_index is graph connectivity in COO format with shape [2, num_edges]\r\n",
    "                edge_index = torch.tensor([row, col], dtype=torch.long)\r\n",
    "                edge_type = torch.tensor(edge_type, dtype=torch.long)\r\n",
    "                # edge_attr is edge feature matrix with shape [num_edges, num_edge_features]\r\n",
    "                edge_attr = F.one_hot(edge_type, num_classes=len(self.bonds)).to(torch.float) \r\n",
    "\r\n",
    "                # order edges based on combined ascending order\r\n",
    "                perm = (edge_index[0] * N + edge_index[1]).argsort() # TODO\r\n",
    "                edge_index = edge_index[:, perm]\r\n",
    "                edge_type = edge_type[perm]\r\n",
    "                edge_attr = edge_attr[perm]\r\n",
    "\r\n",
    "                row, col = edge_index\r\n",
    "                z = torch.tensor(atomic_number, dtype=torch.long)\r\n",
    "                hs = (z == 1).to(torch.float) # hydrogens\r\n",
    "                num_hs = scatter(hs[row], col, dim_size=N).tolist() # scatter helps with one-hot\r\n",
    "                \r\n",
    "                x1 = F.one_hot(torch.tensor(type_idx), num_classes=len(self.types))\r\n",
    "                x2 = torch.tensor([atomic_number, aromatic, sp, sp2, sp3, num_hs], dtype=torch.float).t().contiguous()\r\n",
    "                x = torch.cat([x1.to(torch.float), x2], dim=-1)\r\n",
    "\r\n",
    "                data = Data(x=x, z=z, pos=atom_positions, edge_index=edge_index, edge_attr=edge_attr, idx=i)\r\n",
    "                concat_data_list.append(data)\r\n",
    "\r\n",
    "            # concat_data_list.append(data_list)\r\n",
    "\r\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TEMP_MOLS_LIMIT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-5d1354305316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbase_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'data/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mConcatReactionDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-dc1d01ef5efe>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, pre_transform)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConcatReactionDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\3d-rdkit\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m     53\u001b[0m                  pre_filter=None):\n\u001b[0;32m     54\u001b[0m         super(InMemoryDataset, self).__init__(root, transform, pre_transform,\n\u001b[1;32m---> 55\u001b[1;33m                                               pre_filter)\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__data_list__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\3d-rdkit\\lib\\site-packages\\torch_geometric\\data\\dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'process'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\3d-rdkit\\lib\\site-packages\\torch_geometric\\data\\dataset.py\u001b[0m in \u001b[0;36m_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mosp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessed_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pre_transform.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-dc1d01ef5efe>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \"\"\"\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTEMP_MOLS_LIMIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mconcat_data_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TEMP_MOLS_LIMIT' is not defined"
     ]
    }
   ],
   "source": [
    "base_path = r'data/'\r\n",
    "ConcatReactionDataset(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-f49511064de7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_rp_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(self.collate(concat_rp_dataset), 'test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over epochs\r\n",
    "for epoch in range(max_epochs):\r\n",
    "    # Training\r\n",
    "    for batch, labels in loader:\r\n",
    "        # Transfer to GPU if available\r\n",
    "        batch, labels = batch.to(device), labels.to(device)\r\n",
    "\r\n",
    "        # Model computations\r\n",
    "        [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Data(edge_attr=[2558, 4], idx=[100], pos=[1330, 3], test_neg_edge_index=[2, 255], test_pos_edge_index=[2, 255], train_neg_adj_mask=[1330, 1330], train_pos_edge_index=[2, 146], val_neg_edge_index=[2, 0], val_pos_edge_index=[2, 0], x=[1330, 11], z=[1330])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rp data\r\n",
    "rp_dataset = ReactionDataset(base_path, geo_file = 'train_rp_50')\r\n",
    "rp_data = rp_dataset.data\r\n",
    "rp_data.train_mask = rp_data.val_mask = rp_data.test_mask = rp_data.y = None\r\n",
    "rp_data = train_test_split_edges(data = rp_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "rp_x = rp_data.x.to(device)\r\n",
    "rp_train_pos_edge_index = rp_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# rp autoencoder\r\n",
    "rp_num_node_fs = rp_data.num_node_features\r\n",
    "rp_latent_dim = 2\r\n",
    "rp_ae = GAE(MolEncoder(rp_num_node_fs, rp_latent_dim))\r\n",
    "rp_opt = torch.optim.Adam(rp_ae.parameters(), lr = 0.01)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Reactant, Product) tuples?\r\n",
    "\r\n",
    "Either create a tuple embedding (z_r, z_p) or an embedding of a tuple z_rp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = Data(x=x, z=z, pos=atom_positions, edge_index=edge_index, edge_attr=edge_attr, idx=i)\r\n",
    "# data_list.append(data)\r\n",
    "\r\n",
    "from torch_geometric.data.data import Data\r\n",
    "\r\n",
    "# rp_data_dict = {}\r\n",
    "\r\n",
    "#for dataset_param in r_dataset.data.__dict__.keys():\r\n",
    "#    rp_data_dict[dataset_param] = torch.cat((r_dataset.data[dataset_param], p_dataset.data[dataset_param]))\r\n",
    "\r\n",
    "base_path = r'data/'\r\n",
    "r_dataset = ReactionDataset(base_path, geo_file = 'train_r')\r\n",
    "p_dataset = ReactionDataset(base_path, geo_file = 'train_p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_rp_dataset = [(r_dataset[i], p_dataset[i]) for i in range(len(r_dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reactant-Product Autoencoder\r\n",
    "\r\n",
    "Train reactant and product autoencoders simultaneously and decode to either (i) themselves or (ii) \r\n",
    "\r\n",
    "Do I want to train GAE for R and P separately then combine their latent space and decode to R, say. Or do I want to train R and P together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine R and P on latent space for corresponding features and then decode to R, say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\r\n",
    "from torch_geometric.utils import (negative_sampling, remove_self_loops, add_self_loops)\r\n",
    "EPS = 1e-15\r\n",
    "\r\n",
    "class SameGAE(nn.Module):\r\n",
    "    # r and p in same encoder\r\n",
    "    # either train sequentially i.e. concat r and p data or train on tuples of (r[i], p[i])\r\n",
    "\r\n",
    "    def __init__(self, rp_encoder, decoder, type = \"concat\"):\r\n",
    "        super(SameGAE, self).__init__()\r\n",
    "        self.rp_encoder = rp_encoder\r\n",
    "        self.decoder = decoder\r\n",
    "        self.type = type # \"concat\" or \"tuple\"\r\n",
    "        SameGAE.reset_parameters(self)\r\n",
    "    \r\n",
    "    def reset_parameters(self):\r\n",
    "        reset(self.rp_encoder)\r\n",
    "        reset(self.decoder)\r\n",
    "        reset(self.type)\r\n",
    "    \r\n",
    "    def encode(self, *args, **kwargs):\r\n",
    "        return self.rp_encoder(*args, **kwargs)\r\n",
    "    \r\n",
    "    def decode(self, *args, **kwargs):\r\n",
    "        return self.decoder(*args, **kwargs)\r\n",
    "    \r\n",
    "    def recon_loss(self, z, pos_edge_index, neg_edge_index = None):\r\n",
    "        pos_loss = - torch.log(self.decoder(z, pos_edge_index, sigmoid = True) + EPS).mean()\r\n",
    "\r\n",
    "        # no self-loops in negative samples\r\n",
    "        pos_edge_index, _ = remove_self_loops(pos_edge_index)\r\n",
    "        pos_edge_index, _ = add_self_loops(pos_edge_index)\r\n",
    "        if neg_edge_index is None:\r\n",
    "            neg_edge_index = negative_sampling(pos_edge_index, z.size(0))\r\n",
    "        neg_loss = - torch.log(1 - self.decoder(z, neg_edge_index, sigmoid = True) + EPS).mean()\r\n",
    "\r\n",
    "        return pos_loss + neg_loss\r\n",
    "    \r\n",
    "    def test(self, z, pos_edge_index, neg_edge_index):\r\n",
    "        pos_y = z.new_ones(pos_edge_index.size(1))\r\n",
    "        neg_y = z.new_zeros(neg_edge_index.size(1))\r\n",
    "        y = torch.cat([pos_y, neg_y], dim=0)\r\n",
    "\r\n",
    "        pos_pred = self.decoder(z, pos_edge_index, sigmoid=True)\r\n",
    "        neg_pred = self.decoder(z, neg_edge_index, sigmoid=True)\r\n",
    "        pred = torch.cat([pos_pred, neg_pred], dim=0)\r\n",
    "\r\n",
    "        y, pred = y.detach().cpu().numpy(), pred.detach().cpu().numpy()\r\n",
    "\r\n",
    "        return roc_auc_score(y, pred), average_precision_score(y, pred)\r\n",
    "\r\n",
    "class MolEncoder(nn.Module):\r\n",
    "    def __init__(self, in_channels, out_channels):\r\n",
    "        super(MolEncoder, self).__init__()\r\n",
    "        self.conv = GCNConv(in_channels, out_channels)\r\n",
    "\r\n",
    "    def forward(self, x, edge_index):\r\n",
    "        return self.conv(x, edge_index)\r\n",
    "\r\n",
    "class InnerProductDecoder(nn.Module):\r\n",
    "    def forward(self, z, edge_index, sigmoid = True):\r\n",
    "        value = (z[edge_index[0]] * z[edge_index[1]]).sum(dim = 1)\r\n",
    "        return torch.sigmoid(value) if sigmoid else value\r\n",
    "    \r\n",
    "    def forward_all(self, z, sigmoid = True):\r\n",
    "        adj = torch.matmul(z, z.t())\r\n",
    "        return torch.sigmoid(adj) if sigmoid else adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SameGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p_ae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-dc87af3b7fab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pos_edge_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_neg_edge_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mp_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p_ae' is not defined"
     ]
    }
   ],
   "source": [
    "def train_samegae(gae, opt, x, train_pos_edge_index):\r\n",
    "    gae.train()\r\n",
    "    opt.zero_grad()\r\n",
    "    z = gae.encode(x, train_pos_edge_index)\r\n",
    "    loss = gae.recon_loss(z, train_pos_edge_index)\r\n",
    "    loss.backward()\r\n",
    "    opt.step()\r\n",
    "    return float(loss)\r\n",
    "\r\n",
    "def test_samegae(gae, x, train_pos_edge_index, test_pos_edge_index, test_neg_edge_index):\r\n",
    "    gae.eval()\r\n",
    "    with torch.no_grad():\r\n",
    "        z = gae.encode(x, train_pos_edge_index)\r\n",
    "    return gae.test(z, test_pos_edge_index, test_neg_edge_index)\r\n",
    "\r\n",
    "p_ae.reset_parameters()\r\n",
    "\r\n",
    "epochs = 100\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    loss = train_gae(p_ae, p_opt, p_x, p_data.train_pos_edge_index)\r\n",
    "    auc, ap = test_gae(p_ae, p_x, p_data.train_pos_edge_index, p_data.test_pos_edge_index, p_data.test_neg_edge_index)\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MolEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-56d3557801a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mrp_num_node_fs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrp_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_node_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mrp_latent_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mrp_ae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMolEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrp_num_node_fs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrp_latent_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mrp_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrp_ae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MolEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "# model data\r\n",
    "base_path = r'data/'\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.1 # 50 reactants and 50 products. will end up training on 50 reactants + 40 products and testing on 10 products\r\n",
    "\r\n",
    "# rp data\r\n",
    "rp_dataset = ReactionDataset(base_path, geo_file = 'train_rp_50')\r\n",
    "rp_data = rp_dataset.data\r\n",
    "rp_data.train_mask = rp_data.val_mask = rp_data.test_mask = rp_data.y = None\r\n",
    "rp_data = train_test_split_edges(data = rp_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "rp_x = rp_data.x.to(device)\r\n",
    "rp_train_pos_edge_index = rp_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# rp autoencoder\r\n",
    "rp_num_node_fs = rp_data.num_node_features\r\n",
    "rp_latent_dim = 2\r\n",
    "rp_ae = GAE(MolEncoder(rp_num_node_fs, rp_latent_dim))\r\n",
    "rp_opt = torch.optim.Adam(rp_ae.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['x', 'edge_index', 'edge_attr', 'y', 'pos', 'normal', 'face', 'z', 'idx'])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_dataset.data.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 2486 and 2504 in dimension 1 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-71e67f1b9f80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdataset_param\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mr_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mrp_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_param\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_param\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdataset_param\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 2486 and 2504 in dimension 1 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "# ways of combining:\r\n",
    "#   - combine sdf files (need just first 100) and process that normally\r\n",
    "#   - manually once created datasets\r\n",
    "\r\n",
    "from torch_geometric.data.data import Data\r\n",
    "\r\n",
    "rp_data_dict = {}\r\n",
    "\r\n",
    "for dataset_param in r_dataset.data.__dict__.keys():\r\n",
    "    rp_data_dict[dataset_param] = torch.cat((r_dataset.data[dataset_param], p_dataset.data[dataset_param]))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 0,  ..., 7, 8, 9],\n        [7, 8, 9,  ..., 3, 3, 5]])"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_dataset.data['edge_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2486 2504\n"
     ]
    }
   ],
   "source": [
    "print(len(r_dataset.data.edge_attr), len(p_dataset.data.edge_attr)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "edge_index\n",
      "edge_attr\n",
      "y\n",
      "pos\n",
      "normal\n",
      "face\n",
      "z\n",
      "idx\n"
     ]
    }
   ],
   "source": [
    "for i in r_dataset.data.__dict__.keys():\r\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        ...,\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.],\n        [1., 0., 0., 0.]])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((r_dataset.data['edge_attr'], p_dataset.data['edge_attr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model data\r\n",
    "base_path = r'data/'\r\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "val_ratio = 0\r\n",
    "test_ratio = 0.2\r\n",
    "\r\n",
    "# reactant data\r\n",
    "r_dataset = ReactionDataset(base_path, geo_file = 'train_r') \r\n",
    "r_data = r_dataset.data\r\n",
    "r_data.train_mask = r_data.val_mask = r_data.test_mask = r_data.y = None\r\n",
    "r_data = train_test_split_edges(data = r_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "r_x = r_data.x.to(device)\r\n",
    "r_train_pos_edge_index = r_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# product data\r\n",
    "p_dataset = ReactionDataset(base_path, geo_file = 'train_p') \r\n",
    "p_data = p_dataset.data\r\n",
    "p_data.train_mask = p_data.val_mask = p_data.test_mask = p_data.y = None\r\n",
    "p_data = train_test_split_edges(data = p_data, val_ratio = val_ratio, test_ratio = test_ratio)\r\n",
    "p_x = p_data.x.to(device)\r\n",
    "p_train_pos_edge_index = p_data.train_pos_edge_index.to(device)\r\n",
    "\r\n",
    "# product encoder\r\n",
    "p_num_node_fs = p_data.num_node_features\r\n",
    "p_latent_dim = 2\r\n",
    "p_ae = GAE(MolEncoder(p_num_node_fs, p_latent_dim))\r\n",
    "p_opt = torch.optim.Adam(p_ae.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit (conda)",
   "name": "python3613jvsc74a57bd0f4671ad35fdc0609fa675edcd17de5b3092cb55d03f1d9670a78611a41fb18f3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}